<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ø¢Ù…ÙˆØ²Ø´ Ø¬Ø§Ù…Ø¹ Transformer Architecture</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@300;400;500;700;900&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Vazirmatn', 'Tahoma', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            line-height: 2;
            direction: rtl;
            padding: 20px;
        }
        
        .presentation {
            max-width: 1400px;
            margin: 0 auto;
        }
        
        .slide {
            background: white;
            border-radius: 20px;
            padding: 60px;
            margin-bottom: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            min-height: 600px;
            position: relative;
        }
        
        .slide-number {
            position: absolute;
            top: 20px;
            left: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 1.3em;
        }
        
        h1 {
            color: #667eea;
            font-size: 3em;
            text-align: center;
            margin-bottom: 30px;
            font-weight: 900;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: #667eea;
            font-size: 2.5em;
            margin-bottom: 30px;
            border-right: 8px solid #764ba2;
            padding-right: 20px;
            font-weight: 700;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.8em;
            margin: 30px 0 20px;
            font-weight: 600;
        }
        
        h4 {
            color: #667eea;
            font-size: 1.4em;
            margin: 20px 0 15px;
            font-weight: 500;
        }
        
        p, li {
            font-size: 1.2em;
            margin-bottom: 15px;
            text-align: justify;
            line-height: 2.2;
        }
        
        .subtitle {
            text-align: center;
            font-size: 1.5em;
            color: #764ba2;
            margin-top: 20px;
            font-weight: 500;
        }
        
        .box {
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            border-right: 8px solid;
        }
        
        .info { 
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); 
            border-color: #2196f3; 
        }
        
        .success { 
            background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); 
            border-color: #4caf50; 
        }
        
        .warning { 
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%); 
            border-color: #ff9800; 
        }
        
        .highlight {
            background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
            border-color: #9c27b0;
        }
        
        .code {
            background: #1e293b;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 12px;
            font-family: 'Courier New', 'Consolas', monospace;
            font-size: 1em;
            margin: 25px 0;
            overflow-x: auto;
            direction: ltr;
            text-align: left;
            line-height: 1.8;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }
        
        .code .comment { color: #86efac; }
        .code .keyword { color: #fbbf24; }
        .code .string { color: #fb923c; }
        .code .function { color: #60a5fa; }
        .code .number { color: #c084fc; }
        
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }
        
        .card {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 30px;
            border-radius: 15px;
            border-top: 5px solid #667eea;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .card:hover {
            transform: translateY(-10px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }
        
        .emoji {
            font-size: 1.8em;
            margin-left: 15px;
        }
        
        .highlight-text {
            background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 5px 12px;
            border-radius: 8px;
            font-weight: 700;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            border-radius: 12px;
            overflow: hidden;
        }
        
        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            text-align: right;
            font-weight: 700;
            font-size: 1.1em;
        }
        
        td {
            border: 1px solid #ddd;
            padding: 18px;
            text-align: right;
        }
        
        tr:nth-child(even) { 
            background: #f8f9fa; 
        }
        
        tr:hover {
            background: #e9ecef;
        }
        
        .diagram {
            background: white;
            padding: 40px;
            border-radius: 15px;
            margin: 30px 0;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            text-align: center;
        }
        
        .layer-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px 30px;
            border-radius: 12px;
            margin: 15px auto;
            max-width: 400px;
            font-weight: 600;
            font-size: 1.2em;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .arrow {
            color: #667eea;
            font-size: 2.5em;
            margin: 10px 0;
        }
        
        ul, ol {
            margin-right: 40px;
            margin-bottom: 25px;
        }
        
        li {
            margin-bottom: 15px;
        }
        
        .formula {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
            text-align: center;
            font-size: 1.3em;
            border: 3px solid #667eea;
            font-family: 'Times New Roman', serif;
        }
        
        .timeline {
            position: relative;
            padding: 30px 0;
        }
        
        .timeline-item {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 12px;
            margin-bottom: 25px;
            border-right: 5px solid #667eea;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        
        .timeline-year {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 8px 20px;
            border-radius: 20px;
            display: inline-block;
            font-weight: 700;
            margin-bottom: 15px;
        }
        
        .key-point {
            background: linear-gradient(135deg, #fff9c4 0%, #fff59d 100%);
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
            border-right: 6px solid #fbc02d;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 25px;
            margin: 30px 0;
        }
        
        @media (max-width: 768px) {
            .comparison { grid-template-columns: 1fr; }
            .slide { padding: 30px; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.8em; }
        }
        
        .attention-visual {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            text-align: center;
        }
        
        .matrix {
            display: inline-block;
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 15px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        
        .navigation {
            text-align: center;
            margin-top: 30px;
        }
        
        .nav-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 40px;
            border: none;
            border-radius: 30px;
            font-size: 1.1em;
            font-weight: 700;
            cursor: pointer;
            margin: 0 10px;
            transition: all 0.3s;
        }
        
        .nav-button:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 20px rgba(0,0,0,0.3);
        }
    </style>
</head>
<body>
    <div class="presentation">
        
        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 1: ØµÙØ­Ù‡ Ø¹Ù†ÙˆØ§Ù† -->
        <div class="slide">
            <div class="slide-number">1</div>
            <h1>ğŸ¤– Ù…Ø¹Ù…Ø§Ø±ÛŒ Transformer</h1>
            <p class="subtitle">Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ</p>
            
            <div style="text-align: center; margin-top: 80px;">
                <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='200' height='200' viewBox='0 0 200 200'%3E%3Ccircle cx='100' cy='100' r='80' fill='%23667eea' opacity='0.2'/%3E%3Ccircle cx='100' cy='100' r='60' fill='%23764ba2' opacity='0.3'/%3E%3Ccircle cx='100' cy='100' r='40' fill='%23667eea' opacity='0.4'/%3E%3C/svg%3E" alt="Transformer">
            </div>
            
            <div class="info box" style="margin-top: 60px;">
                <h3><span class="emoji">ğŸ“–</span> Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø§Ø±Ø§Ø¦Ù‡</h3>
                <p>Ø¯Ø± Ø§ÛŒÙ† Ø§Ø±Ø§Ø¦Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Transformer Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯ - Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ù‡ Ù¾Ø§ÛŒÙ‡ Ùˆ Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø§Ù†Ù†Ø¯ GPTØŒ BERTØŒ Ùˆ Claude Ø§Ø³Øª.</p>
            </div>
            
            <div class="key-point">
                <p style="text-align: center; font-size: 1.3em; font-weight: 600;">
                    <span class="emoji">ğŸ¯</span> Ù‡Ø¯Ù: Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø± Transformer Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù†
                </p>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 2: ØªØ§Ø±ÛŒØ®Ú†Ù‡ -->
        <div class="slide">
            <div class="slide-number">2</div>
            <h2>ğŸ“œ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ùˆ ØªÚ©Ø§Ù…Ù„</h2>
            
            <div class="timeline">
                <div class="timeline-item">
                    <span class="timeline-year">Ù‚Ø¨Ù„ Ø§Ø² 2017</span>
                    <h4>Ø¯ÙˆØ±Ø§Ù† RNN Ùˆ LSTM</h4>
                    <p>Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (Recurrent) Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯. Ù…Ø´Ú©Ù„Ø§Øª Ø§ØµÙ„ÛŒ:</p>
                    <ul>
                        <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ±ØªÛŒØ¨ÛŒ Ùˆ Ú©Ù†Ø¯</li>
                        <li>Ù…Ø´Ú©Ù„ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª</li>
                        <li>Ø¹Ø¯Ù… Ø§Ù…Ú©Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ</li>
                        <li>Gradient Vanishing</li>
                    </ul>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2017</span>
                    <h4>Ø§Ù†ØªØ´Ø§Ø± Ù…Ù‚Ø§Ù„Ù‡ "Attention Is All You Need"</h4>
                    <p>Ù…Ø­Ù‚Ù‚Ø§Ù† Google (Vaswani et al.) Ù…Ø¹Ù…Ø§Ø±ÛŒ Transformer Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ ØªÙ…Ø§Ù… Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø­Ù„ Ú©Ø±Ø¯.</p>
                    <ul>
                        <li>Ø­Ø°Ù Ú©Ø§Ù…Ù„ RNN Ùˆ LSTM</li>
                        <li>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ú©Ø§Ù†ÛŒØ²Ù… Self-Attention</li>
                        <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù…Ù„</li>
                        <li>Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ</li>
                    </ul>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2018-2019</span>
                    <h4>Ø¸Ù‡ÙˆØ± BERT Ùˆ GPT</h4>
                    <p>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Transformer Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø´Ø¯Ù‡:</p>
                    <ul>
                        <li><strong>BERT:</strong> Bidirectional Encoder</li>
                        <li><strong>GPT:</strong> Autoregressive Decoder</li>
                        <li>Ù†ØªØ§ÛŒØ¬ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø¯Ø± ØªÙ…Ø§Ù… ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ NLP</li>
                    </ul>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2020-Ø­Ø§Ù„</span>
                    <h4>Ø§Ù†ÙØ¬Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯</h4>
                    <p>ØªÚ©Ø§Ù…Ù„ Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÙˆÙ„â€ŒÙ¾ÛŒÚ©Ø±:</p>
                    <ul>
                        <li>GPT-3, GPT-4 (OpenAI)</li>
                        <li>Claude (Anthropic)</li>
                        <li>PaLM (Google)</li>
                        <li>LLaMA (Meta)</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 3: Ú†Ø±Ø§ TransformerØŸ -->
        <div class="slide">
            <div class="slide-number">3</div>
            <h2>ğŸ¤” Ú†Ø±Ø§ Transformer Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ø¨ÙˆØ¯ØŸ</h2>
            
            <div class="comparison">
                <div class="warning box">
                    <h3><span class="emoji">âŒ</span> Ù…Ø´Ú©Ù„Ø§Øª RNN/LSTM</h3>
                    <ul>
                        <li><strong>Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ±ØªÛŒØ¨ÛŒ:</strong> Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø±Ø¯</li>
                        <li><strong>Ø­Ø§ÙØ¸Ù‡ Ù…Ø­Ø¯ÙˆØ¯:</strong> Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÙˆØ± Ø§Ø² Ù‡Ù… ÙØ±Ø§Ù…ÙˆØ´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯</li>
                        <li><strong>Ú©Ù†Ø¯ÛŒ:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù„Ù†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ú©Ù†Ø¯ Ø§Ø³Øª</li>
                        <li><strong>Gradient Vanishing:</strong> Ù…Ø´Ú©Ù„ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÙˆØ±</li>
                    </ul>
                </div>
                
                <div class="success box">
                    <h3><span class="emoji">âœ…</span> Ù…Ø²Ø§ÛŒØ§ÛŒ Transformer</h3>
                    <ul>
                        <li><strong>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ:</strong> ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ù‡Ù…Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯</li>
                        <li><strong>ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ú©Ù„ Ù…ØªÙ†:</strong> Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¨Ù‡ ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ø¯ÛŒÚ¯Ø± ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li><strong>Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§:</strong> Ø¨Ø§ GPU Ù‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹ Ø§Ø³Øª</li>
                        <li><strong>Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ:</strong> Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ Ø³Ø§Ø®Øª</li>
                    </ul>
                </div>
            </div>
            
            <div class="key-point">
                <h3><span class="emoji">ğŸ’¡</span> Ù†Ú©ØªÙ‡ Ú©Ù„ÛŒØ¯ÛŒ</h3>
                <p>Transformer Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ú©Ø§Ù†ÛŒØ²Ù… <span class="highlight-text">Self-Attention</span> Ø¨Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ø¯ÛŒÚ¯Ø± Ø¯Ø± Ø¬Ù…Ù„Ù‡ ØªÙˆØ¬Ù‡ Ú©Ù†Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¹Ø¨ÙˆØ± Ø§Ø² Ù…Ø±Ø§Ø­Ù„ Ù…ÛŒØ§Ù†ÛŒ.</p>
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ“Š</span> Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ:</h4>
                <p>Ø¬Ù…Ù„Ù‡: "Ø¨Ø§Ù†Ú© Ø±ÙˆØ¯Ø®Ø§Ù†Ù‡ Ù¾Ø± Ø§Ø² Ù…Ø§Ù‡ÛŒ Ø¨ÙˆØ¯"</p>
                <ul>
                    <li><strong>RNN:</strong> Ù…Ù…Ú©Ù† Ø§Ø³Øª "Ø¨Ø§Ù†Ú©" Ø±Ø§ Ø¨Ø§ "Ù¾ÙˆÙ„" Ù…Ø±ØªØ¨Ø· Ú©Ù†Ø¯</li>
                    <li><strong>Transformer:</strong> Ø¨Ø§ Ø¯ÛŒØ¯Ù† "Ø±ÙˆØ¯Ø®Ø§Ù†Ù‡"ØŒ Ù…ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ "Ø¨Ø§Ù†Ú©" Ø¨Ù‡ Ù…Ø¹Ù†Ø§ÛŒ "Ú©Ù†Ø§Ø± Ø±ÙˆØ¯Ø®Ø§Ù†Ù‡" Ø§Ø³Øª</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 4: Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ù„ÛŒ -->
        <div class="slide">
            <div class="slide-number">4</div>
            <h2>ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ù„ÛŒ Transformer</h2>
            
            <div class="diagram">
                <h3 style="color: #667eea; margin-bottom: 30px;">Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ</h3>
                
                <div class="layer-box">ÙˆØ±ÙˆØ¯ÛŒ (Input)</div>
                <div class="arrow">â†“</div>
                
                <div class="layer-box">Input Embedding + Positional Encoding</div>
                <div class="arrow">â†“</div>
                
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    Encoder<br>
                    <span style="font-size: 0.8em;">(N Ù„Ø§ÛŒÙ‡)</span>
                </div>
                <div class="arrow">â†“</div>
                
                <div class="layer-box" style="background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);">
                    Decoder<br>
                    <span style="font-size: 0.8em;">(N Ù„Ø§ÛŒÙ‡)</span>
                </div>
                <div class="arrow">â†“</div>
                
                <div class="layer-box">Linear + Softmax</div>
                <div class="arrow">â†“</div>
                
                <div class="layer-box">Ø®Ø±ÙˆØ¬ÛŒ (Output)</div>
            </div>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸ“¥</span> Encoder</h4>
                    <p>ÙˆØ¸ÛŒÙÙ‡: Ø¯Ø±Ú© Ùˆ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ±ÙˆØ¯ÛŒ</p>
                    <ul>
                        <li>Multi-Head Attention</li>
                        <li>Feed Forward Network</li>
                        <li>Layer Normalization</li>
                        <li>Residual Connections</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“¤</span> Decoder</h4>
                    <p>ÙˆØ¸ÛŒÙÙ‡: ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ</p>
                    <ul>
                        <li>Masked Multi-Head Attention</li>
                        <li>Encoder-Decoder Attention</li>
                        <li>Feed Forward Network</li>
                        <li>Layer Normalization</li>
                    </ul>
                </div>
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ”</span> ØªÙØ§ÙˆØª Ø§ØµÙ„ÛŒ:</h4>
                <p>Ø¯Ø± Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒØŒ <strong>Encoder</strong> Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ <strong>Decoder</strong> ØªØ±Ø¬Ù…Ù‡ ÛŒØ§ Ù¾Ø§Ø³Ø® Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ù…Ø§ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ø±Ù†:</p>
                <ul>
                    <li><strong>BERT:</strong> ÙÙ‚Ø· Ø§Ø² Encoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                    <li><strong>GPT:</strong> ÙÙ‚Ø· Ø§Ø² Decoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                    <li><strong>T5:</strong> Ø§Ø² Ù‡Ø± Ø¯Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 5: Embedding -->
        <div class="slide">
            <div class="slide-number">5</div>
            <h2>ğŸ”¤ Input Embedding</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ“š</span> Embedding Ú†ÛŒØ³ØªØŸ</h3>
                <p>ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ù‡ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¨ØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ø§Ø± Ú©Ù†Ø¯.</p>
            </div>
            
            <div class="highlight box">
                <h4><span class="emoji">ğŸ’¡</span> Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡:</h4>
                <p>Ú©Ù„Ù…Ù‡ "Ú¯Ø±Ø¨Ù‡" â†’ Ø¨Ø±Ø¯Ø§Ø± [0.2, -0.5, 0.8, 0.1, ...]</p>
                <p>Ú©Ù„Ù…Ù‡ "Ø³Ú¯" â†’ Ø¨Ø±Ø¯Ø§Ø± [0.3, -0.4, 0.7, 0.2, ...]</p>
                <p>Ú©Ù„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù‡Ù… Ø¯Ø§Ø±Ù†Ø¯!</p>
            </div>
            
            <div class="code">
<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn

<span class="keyword">class</span> <span class="function">Embedding</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model):
        <span class="string">"""
        vocab_size: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÙˆØ§Ú˜Ú¯Ø§Ù†
        d_model: Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø± embedding (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 512 ÛŒØ§ 768)
        """</span>
        self.embedding = nn.Embedding(vocab_size, d_model)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±</span>
        <span class="keyword">return</span> self.embedding(x) * torch.sqrt(torch.tensor(d_model))

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
vocab_size = <span class="number">10000</span>  <span class="comment"># 10 Ù‡Ø²Ø§Ø± Ú©Ù„Ù…Ù‡</span>
d_model = <span class="number">512</span>  <span class="comment"># Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø±</span>

emb = Embedding(vocab_size, d_model)

<span class="comment"># Ø¬Ù…Ù„Ù‡: "Ù…Ù† ÛŒÚ© Ø¯Ø§Ù†Ø´Ø¬Ùˆ Ù‡Ø³ØªÙ…"</span>
<span class="comment"># Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§: [45, 102, 789, 1200]</span>
sentence = torch.tensor([<span class="number">45</span>, <span class="number">102</span>, <span class="number">789</span>, <span class="number">1200</span>])
embedded = emb.forward(sentence)
<span class="comment"># Ø®Ø±ÙˆØ¬ÛŒ: tensor Ø´Ú©Ù„ [4, 512]</span>
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ¯</span> Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h4>
                <ul>
                    <li>Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¨Ø§ Ø§Ø¨Ø¹Ø§Ø¯ Ø«Ø§Ø¨Øª (d_model) ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>
                    <li>Ø§ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯</li>
                    <li>Ú©Ù„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒØŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø¯Ø§Ø±Ù†Ø¯</li>
                    <li>Ø¶Ø±Ø¨ Ø¯Ø± âˆšd_model Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³Øª</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 6: Positional Encoding -->
        <div class="slide">
            <div class="slide-number">6</div>
            <h2>ğŸ“ Positional Encoding</h2>
            
            <div class="warning box">
                <h3><span class="emoji">âš ï¸</span> Ù…Ø´Ú©Ù„:</h3>
                <p>Transformer Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù¾Ø³ ØªØ±ØªÛŒØ¨ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯!</p>
                <p>Ù…Ø«Ø§Ù„: "Ø³Ú¯ Ú¯Ø±Ø¨Ù‡ Ø±Ø§ Ø¯ÛŒØ¯" Ùˆ "Ú¯Ø±Ø¨Ù‡ Ø³Ú¯ Ø±Ø§ Ø¯ÛŒØ¯" Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ÛŒÚ©Ø³Ø§Ù† Ø§Ø³Øª! ğŸ˜±</p>
            </div>
            
            <div class="success box">
                <h3><span class="emoji">âœ…</span> Ø±Ø§Ù‡â€ŒØ­Ù„: Positional Encoding</h3>
                <p>Ø¨Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ ÛŒÚ© "Ú©Ø¯ Ù…ÙˆÙ‚Ø¹ÛŒØª" Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù…Ø¯Ù„ Ø¨ÙÙ‡Ù…Ø¯ Ú©Ù„Ù…Ù‡ Ø¯Ø± Ú©Ø¬Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.</p>
            </div>
            
            <div class="formula">
                PE<sub>(pos, 2i)</sub> = sin(pos / 10000<sup>2i/d_model</sup>)<br><br>
                PE<sub>(pos, 2i+1)</sub> = cos(pos / 10000<sup>2i/d_model</sup>)
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ“</span> ØªÙˆØ¶ÛŒØ­ ÙØ±Ù…ÙˆÙ„:</h4>
                <ul>
                    <li><strong>pos:</strong> Ù…ÙˆÙ‚Ø¹ÛŒØª Ú©Ù„Ù…Ù‡ Ø¯Ø± Ø¬Ù…Ù„Ù‡ (0ØŒ 1ØŒ 2ØŒ ...)</li>
                    <li><strong>i:</strong> Ø´Ù…Ø§Ø±Ù‡ Ø¨Ø¹Ø¯ Ø¯Ø± Ø¨Ø±Ø¯Ø§Ø±</li>
                    <li><strong>d_model:</strong> Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø± embedding</li>
                    <li>Ø§Ø² ØªÙˆØ§Ø¨Ø¹ sin Ùˆ cos Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø§Ù„Ú¯ÙˆÛŒÛŒ Ù…Ù†Ø­ØµØ±Ø¨ÙØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ¯</li>
                </ul>
            </div>
            
            <div class="code">
<span class="keyword">import</span> torch
<span class="keyword">import</span> math

<span class="keyword">class</span> <span class="function">PositionalEncoding</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, max_len=<span class="number">5000</span>):
        <span class="string">"""
        d_model: Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø±
        max_len: Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡
        """</span>
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)
        
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ div_term</span>
        div_term = torch.exp(
            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * 
            -(math.log(<span class="number">10000.0</span>) / d_model)
        )
        
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ sin Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¬</span>
        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)
        
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ cos Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ÛŒ ÙØ±Ø¯</span>
        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)
        
        self.pe = pe.unsqueeze(<span class="number">0</span>)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† positional encoding Ø¨Ù‡ embedding</span>
        <span class="keyword">return</span> x + self.pe[:, :x.size(<span class="number">1</span>)]

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
d_model = <span class="number">512</span>
pos_enc = PositionalEncoding(d_model)

<span class="comment"># ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ embedded_sentence Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ù‚Ø¨Ù„ Ø¯Ø§Ø±ÛŒÙ…</span>
<span class="comment"># Ø´Ú©Ù„: [batch_size, seq_len, d_model]</span>
final_input = pos_enc.forward(embedded_sentence)
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ¯</span> Ú†Ø±Ø§ Ø§ÛŒÙ† Ø±ÙˆØ´ØŸ</h4>
                <ul>
                    <li><strong>Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯:</strong> Ù‡Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª ÛŒÚ© Ø§Ù„Ú¯ÙˆÛŒ ÛŒÚ©ØªØ§ Ø¯Ø§Ø±Ø¯</li>
                    <li><strong>Ù‚Ø§Ø¨Ù„ ØªØ¹Ù…ÛŒÙ…:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù„Ù†Ø¯ØªØ± Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                    <li><strong>Ù†Ø³Ø¨ÛŒ:</strong> Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨ÙÙ‡Ù…Ø¯</li>
                    <li><strong>Ø«Ø§Ø¨Øª:</strong> Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ù†Ø¯Ø§Ø±Ø¯</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 7: Self-Attention -->
        <div class="slide">
            <div class="slide-number">7</div>
            <h2>ğŸ‘ï¸ Self-Attention: Ù‚Ù„Ø¨ Transformer</h2>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ§ </span> Self-Attention Ú†ÛŒØ³ØªØŸ</h3>
                <p>Ù…Ú©Ø§Ù†ÛŒØ²Ù…ÛŒ Ú©Ù‡ Ø¨Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¨Ù‡ ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ø¯ÛŒÚ¯Ø± Ø¯Ø± Ø¬Ù…Ù„Ù‡ "ØªÙˆØ¬Ù‡" Ú©Ù†Ø¯ Ùˆ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±Ø¯ Ú©Ø¯Ø§Ù… Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù…â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯.</p>
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ’¡</span> Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡:</h4>
                <p><strong>Ø¬Ù…Ù„Ù‡:</strong> "Ø­ÛŒÙˆØ§Ù† Ø®Ø³ØªÙ‡ Ø¨ÙˆØ¯ Ú†ÙˆÙ† Ø±Ø§Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ø±ÙØªÙ‡ Ø¨ÙˆØ¯"</p>
                <p>ÙˆÙ‚ØªÛŒ Ù…Ø¯Ù„ Ú©Ù„Ù…Ù‡ "Ø­ÛŒÙˆØ§Ù†" Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:</p>
                <ul>
                    <li>Ø¨Ù‡ "Ø®Ø³ØªÙ‡" ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ú†ÙˆÙ† ØµÙØª Ø­ÛŒÙˆØ§Ù† Ø§Ø³Øª)</li>
                    <li>Ø¨Ù‡ "Ø±ÙØªÙ‡" ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ú†ÙˆÙ† ÙØ¹Ù„ Ø­ÛŒÙˆØ§Ù† Ø§Ø³Øª)</li>
                    <li>Ø¨Ù‡ "Ø±Ø§Ù‡" Ú©Ù…ØªØ± ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                </ul>
            </div>
            
            <h3 style="margin-top: 40px;">ğŸ”‘ Ø³Ù‡ Ù…ÙÙ‡ÙˆÙ… Ú©Ù„ÛŒØ¯ÛŒ: QØŒ KØŒ V</h3>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">â“</span> Query (Ù¾Ø±Ø³Ø´)</h4>
                    <p>Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ù†Ø¯.</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> "Ù…Ù† Ú†Ù‡ Ú©Ù„Ù…Ø§ØªÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø¨ÛŒÙ†Ù…ØŸ"</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ”‘</span> Key (Ú©Ù„ÛŒØ¯)</h4>
                    <p>Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù‡Ø± Ú©Ù„Ù…Ù‡ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø§Ø±Ø¯.</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> "Ù…Ù† ÛŒÚ© ÙØ¹Ù„ Ù‡Ø³ØªÙ…"</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’</span> Value (Ù…Ù‚Ø¯Ø§Ø±)</h4>
                    <p>Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ Ú©Ù„Ù…Ù‡ Ø­Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Ù…Ø¹Ù†Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù„Ù…Ù‡</p>
                </div>
            </div>
            
            <div class="formula">
                Attention(Q, K, V) = softmax(QK<sup>T</sup> / âˆšd<sub>k</sub>) V
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ“</span> ØªÙˆØ¶ÛŒØ­ ÙØ±Ù…ÙˆÙ„ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù…:</h4>
                <ol>
                    <li><strong>QK<sup>T</sup>:</strong> Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Query Ùˆ Key Ù‡Ø§</li>
                    <li><strong>/ âˆšd<sub>k</sub>:</strong> Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø¹Ø¯Ø§Ø¯ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯)</li>
                    <li><strong>softmax:</strong> ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ (Ù…Ø¬Ù…ÙˆØ¹ = 1)</li>
                    <li><strong>Ã— V:</strong> Ø¶Ø±Ø¨ Ø¯Ø± Value Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ</li>
                </ol>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 8: Ú©Ø¯ Self-Attention -->
        <div class="slide">
            <div class="slide-number">8</div>
            <h2>ğŸ’» Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Self-Attention</h2>
            
            <div class="code">
<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F
<span class="keyword">import</span> math

<span class="keyword">class</span> <span class="function">SelfAttention</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model):
        <span class="string">"""
        d_model: Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒ
        """</span>
        <span class="keyword">super</span>().__init__()
        self.d_model = d_model
        
        <span class="comment"># Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ø§ÛŒ QØŒ KØŒ V</span>
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        
        <span class="comment"># Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ</span>
        self.out = nn.Linear(d_model, d_model)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        <span class="string">"""
        x: ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ [batch_size, seq_len, d_model]
        mask: Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¨Ø±Ø®ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§
        """</span>
        batch_size, seq_len, d_model = x.size()
        
        <span class="comment"># Ú¯Ø§Ù… 1: Ù…Ø­Ø§Ø³Ø¨Ù‡ QØŒ KØŒ V</span>
        Q = self.query(x)  <span class="comment"># [batch, seq_len, d_model]</span>
        K = self.key(x)    <span class="comment"># [batch, seq_len, d_model]</span>
        V = self.value(x)  <span class="comment"># [batch, seq_len, d_model]</span>
        
        <span class="comment"># Ú¯Ø§Ù… 2: Ù…Ø­Ø§Ø³Ø¨Ù‡ Attention Scores</span>
        <span class="comment"># QK^T</span>
        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))
        <span class="comment"># [batch, seq_len, seq_len]</span>
        
        <span class="comment"># Ú¯Ø§Ù… 3: Scale (ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø¬Ø°Ø± d_model)</span>
        scores = scores / math.sqrt(d_model)
        
        <span class="comment"># Ú¯Ø§Ù… 4: Ø§Ø¹Ù…Ø§Ù„ Mask (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)</span>
        <span class="keyword">if</span> mask <span class="keyword">is not None</span>:
            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)
        
        <span class="comment"># Ú¯Ø§Ù… 5: Softmax</span>
        attention_weights = F.softmax(scores, dim=-<span class="number">1</span>)
        <span class="comment"># [batch, seq_len, seq_len]</span>
        
        <span class="comment"># Ú¯Ø§Ù… 6: Ø¶Ø±Ø¨ Ø¯Ø± V</span>
        output = torch.matmul(attention_weights, V)
        <span class="comment"># [batch, seq_len, d_model]</span>
        
        <span class="comment"># Ú¯Ø§Ù… 7: Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ</span>
        output = self.out(output)
        
        <span class="keyword">return</span> output, attention_weights

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
d_model = <span class="number">512</span>
seq_len = <span class="number">10</span>
batch_size = <span class="number">32</span>

attention = SelfAttention(d_model)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = attention.forward(x)
<span class="keyword">print</span>(<span class="string">f"Output shape: {output.shape}"</span>)
<span class="keyword">print</span>(<span class="string">f"Attention weights shape: {weights.shape}"</span>)
            </div>
            
            <div class="success box">
                <h4><span class="emoji">âœ…</span> Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ú©Ø¯:</h4>
                <ul>
                    <li><strong>nn.Linear:</strong> Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ ÙˆØ²Ù† Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ QØŒ KØŒ V</li>
                    <li><strong>transpose:</strong> Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¶Ø±Ø¨ Ù…Ø§ØªØ±ÛŒØ³ÛŒ QK<sup>T</sup></li>
                    <li><strong>masked_fill:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ</li>
                    <li><strong>softmax:</strong> ØªØ¨Ø¯ÛŒÙ„ scores Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„</li>
                    <li><strong>attention_weights:</strong> Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ø¯Ø§Ù… Ú©Ù„Ù…Ø§Øª ØªÙˆØ¬Ù‡ Ú©Ø±Ø¯Ù‡</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 9: Multi-Head Attention -->
        <div class="slide">
            <div class="slide-number">9</div>
            <h2>ğŸ­ Multi-Head Attention</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ¤”</span> Ú†Ø±Ø§ Multi-HeadØŸ</h3>
                <p>Ø¨Ù‡ Ø¬Ø§ÛŒ ÛŒÚ© attentionØŒ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† attention Ù…ÙˆØ§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ù‡Ø± "head" Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¬Ù…Ù„Ù‡ ØªÙˆØ¬Ù‡ Ú©Ù†Ø¯:</p>
                <ul>
                    <li><strong>Head 1:</strong> Ø±ÙˆØ§Ø¨Ø· Ø¯Ø³ØªÙˆØ±ÛŒ (ÙØ§Ø¹Ù„ - ÙØ¹Ù„)</li>
                    <li><strong>Head 2:</strong> Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ (ØµÙØª - Ù…ÙˆØµÙˆÙ)</li>
                    <li><strong>Head 3:</strong> ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÙˆØ±</li>
                    <li><strong>Head 4:</strong> Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø²Ø¨Ø§Ù†</li>
                </ul>
            </div>
            
            <div class="diagram">
                <h3 style="color: #667eea; margin-bottom: 30px;">Ø³Ø§Ø®ØªØ§Ø± Multi-Head Attention</h3>
                
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    ÙˆØ±ÙˆØ¯ÛŒ (Input)
                </div>
                <div class="arrow">â†“</div>
                
                <div style="display: flex; justify-content: space-around; margin: 20px 0;">
                    <div class="layer-box" style="max-width: 150px; font-size: 0.9em;">Head 1</div>
                    <div class="layer-box" style="max-width: 150px; font-size: 0.9em;">Head 2</div>
                    <div class="layer-box" style="max-width: 150px; font-size: 0.9em;">Head 3</div>
                    <div class="layer-box" style="max-width: 150px; font-size: 0.9em;">Head 4</div>
                </div>
                
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);">
                    Concatenate
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Ø®Ø±ÙˆØ¬ÛŒ (Output)</div>
            </div>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">MultiHeadAttention</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads):
        <span class="string">"""
        d_model: Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø± Ú©Ù„ (Ø¨Ø§ÛŒØ¯ Ø¨Ø± num_heads Ø¨Ø®Ø´â€ŒÙ¾Ø°ÛŒØ± Ø¨Ø§Ø´Ø¯)
        num_heads: ØªØ¹Ø¯Ø§Ø¯ attention heads (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 8)
        """</span>
        <span class="keyword">super</span>().__init__()
        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  <span class="comment"># Ø¨Ø¹Ø¯ Ù‡Ø± head</span>
        
        <span class="comment"># Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„</span>
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    <span class="keyword">def</span> <span class="function">split_heads</span>(self, x):
        <span class="string">"""ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú†Ù†Ø¯ head"""</span>
        batch_size, seq_len, d_model = x.size()
        <span class="keyword">return</span> x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        batch_size = x.size(<span class="number">0</span>)
        
        <span class="comment"># 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ QØŒ KØŒ V Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… heads</span>
        Q = self.split_heads(self.W_q(x))
        K = self.split_heads(self.W_k(x))
        V = self.split_heads(self.W_v(x))
        <span class="comment"># Ø´Ú©Ù„: [batch, num_heads, seq_len, d_k]</span>
        
        <span class="comment"># 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ attention</span>
        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)
        
        <span class="keyword">if</span> mask <span class="keyword">is not None</span>:
            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)
        
        attention = F.softmax(scores, dim=-<span class="number">1</span>)
        output = torch.matmul(attention, V)
        
        <span class="comment"># 3. ØªØ±Ú©ÛŒØ¨ heads</span>
        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()
        output = output.view(batch_size, -<span class="number">1</span>, self.d_model)
        
        <span class="comment"># 4. Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ</span>
        <span class="keyword">return</span> self.W_o(output)

<span class="comment"># Ù…Ø«Ø§Ù„</span>
d_model = <span class="number">512</span>
num_heads = <span class="number">8</span>
mha = MultiHeadAttention(d_model, num_heads)

x = torch.randn(<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>)
output = mha.forward(x)
<span class="keyword">print</span>(<span class="string">f"Output shape: {output.shape}"</span>)  <span class="comment"># [32, 10, 512]</span>
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ¯</span> Ù…Ø²Ø§ÛŒØ§ÛŒ Multi-Head:</h4>
                <ul>
                    <li>Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù‡Ù…Ø²Ù…Ø§Ù† ØªÙˆØ¬Ù‡ Ú©Ù†Ø¯</li>
                    <li>Ø§ÙØ²Ø§ÛŒØ´ Ù‚Ø¯Ø±Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø¯ÙˆÙ† Ø§ÙØ²Ø§ÛŒØ´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§</li>
                    <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ù‡ØªØ±</li>
                    <li>Ú©Ø§Ù‡Ø´ overfitting</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 10: Feed Forward Network -->
        <div class="slide">
            <div class="slide-number">10</div>
            <h2>ğŸ”„ Feed Forward Network</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ§ </span> FFN Ú†ÛŒØ³ØªØŸ</h3>
                <p>Ø¨Ø¹Ø¯ Ø§Ø² attentionØŒ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø§Ø² ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø³Ø§Ø¯Ù‡ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ø´Ø¨Ú©Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ù‡ØªØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ø¯.</p>
            </div>
            
            <div class="diagram">
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    ÙˆØ±ÙˆØ¯ÛŒ (d_model)
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Linear Layer 1<br>(d_model â†’ d_ff)</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);">
                    ReLU Activation
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Linear Layer 2<br>(d_ff â†’ d_model)</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    Ø®Ø±ÙˆØ¬ÛŒ (d_model)
                </div>
            </div>
            
            <div class="formula">
                FFN(x) = max(0, xW<sub>1</sub> + b<sub>1</sub>)W<sub>2</sub> + b<sub>2</sub>
            </div>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">FeedForward</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span>):
        <span class="string">"""
        d_model: Ø¨Ø¹Ø¯ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ
        d_ff: Ø¨Ø¹Ø¯ Ù„Ø§ÛŒÙ‡ Ù…ÛŒØ§Ù†ÛŒ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 4 Ø¨Ø±Ø§Ø¨Ø± d_model)
        dropout: Ù†Ø±Ø® dropout Ø¨Ø±Ø§ÛŒ regularization
        """</span>
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„: Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø¨Ø¹Ø§Ø¯</span>
        self.linear1 = nn.Linear(d_model, d_ff)
        
        <span class="comment"># ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ</span>
        self.relu = nn.ReLU()
        
        <span class="comment"># Dropout Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting</span>
        self.dropout = nn.Dropout(dropout)
        
        <span class="comment"># Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…: Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ Ø§ØµÙ„ÛŒ</span>
        self.linear2 = nn.Linear(d_ff, d_model)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="string">"""
        x: ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ [batch_size, seq_len, d_model]
        """</span>
        <span class="comment"># Ú¯Ø§Ù… 1: Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„ + ReLU</span>
        x = self.relu(self.linear1(x))
        <span class="comment"># [batch, seq_len, d_ff]</span>
        
        <span class="comment"># Ú¯Ø§Ù… 2: Dropout</span>
        x = self.dropout(x)
        
        <span class="comment"># Ú¯Ø§Ù… 3: Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…</span>
        x = self.linear2(x)
        <span class="comment"># [batch, seq_len, d_model]</span>
        
        <span class="keyword">return</span> x

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
d_model = <span class="number">512</span>
d_ff = <span class="number">2048</span>

ffn = FeedForward(d_model, d_ff)
x = torch.randn(<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>)
output = ffn.forward(x)
<span class="keyword">print</span>(<span class="string">f"Output shape: {output.shape}"</span>)  <span class="comment"># [32, 10, 512]</span>
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ’¡</span> Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h4>
                <ul>
                    <li><strong>d_ff Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 4 Ø¨Ø±Ø§Ø¨Ø± d_model Ø§Ø³Øª:</strong> Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¸Ø±ÙÛŒØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ</li>
                    <li><strong>ReLU:</strong> ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø³Ø±ÛŒØ¹</li>
                    <li><strong>Position-wise:</strong> Ù‡Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø³ØªÙ‚Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>
                    <li><strong>Dropout:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 11: Layer Normalization -->
        <div class="slide">
            <div class="slide-number">11</div>
            <h2>ğŸ“Š Layer Normalization & Residual Connections</h2>
            
            <div class="grid">
                <div class="info box">
                    <h3><span class="emoji">ğŸ“</span> Layer Normalization</h3>
                    <p>Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Ø±ÛŒØ¹ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´.</p>
                    <div class="formula" style="font-size: 1em; margin-top: 15px;">
                        LN(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
                    </div>
                    <ul style="margin-top: 15px;">
                        <li>Î¼: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†</li>
                        <li>Ïƒ: Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±</li>
                        <li>Î³ØŒ Î²: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´</li>
                    </ul>
                </div>
                
                <div class="success box">
                    <h3><span class="emoji">ğŸ”—</span> Residual Connection</h3>
                    <p>Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Vanishing Gradient.</p>
                    <div class="formula" style="font-size: 1em; margin-top: 15px;">
                        output = LayerNorm(x + Sublayer(x))
                    </div>
                    <p style="margin-top: 15px;"><strong>Ù…Ø²ÛŒØª:</strong> Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ gradient Ø¨Ù‡ØªØ± Ø¬Ø±ÛŒØ§Ù† Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.</p>
                </div>
            </div>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">LayerNorm</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, eps=<span class="number">1e-6</span>):
        <span class="keyword">super</span>().__init__()
        <span class="comment"># Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´</span>
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³</span>
        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="keyword">True</span>)
        std = x.std(-<span class="number">1</span>, keepdim=<span class="keyword">True</span>)
        
        <span class="comment"># Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ</span>
        <span class="keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta

<span class="keyword">class</span> <span class="function">SublayerConnection</span>(nn.Module):
    <span class="string">"""
    Residual connection + Layer Normalization
    """</span>
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        self.norm = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, sublayer):
        <span class="string">"""
        x: ÙˆØ±ÙˆØ¯ÛŒ
        sublayer: ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†ÛŒÙ… (Ù…Ø«Ù„ attention)
        """</span>
        <span class="comment"># Add & Norm</span>
        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
d_model = <span class="number">512</span>
sublayer_conn = SublayerConnection(d_model)

<span class="comment"># ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ attention_layer ÛŒÚ© Ù„Ø§ÛŒÙ‡ attention Ø§Ø³Øª</span>
x = torch.randn(<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>)
output = sublayer_conn.forward(x, <span class="keyword">lambda</span> x: attention_layer(x))
            </div>
            
            <div class="diagram">
                <h4 style="color: #667eea; text-align: center; margin-bottom: 20px;">Ø¬Ø±ÛŒØ§Ù† Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÛŒÚ© Ù„Ø§ÛŒÙ‡ Encoder</h4>
                
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    ÙˆØ±ÙˆØ¯ÛŒ (x)
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Layer Norm</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #2196f3 0%, #1976d2 100%);">
                    Multi-Head Attention
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);">
                    Add (Residual) + Dropout
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Layer Norm</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #9c27b0 0%, #7b1fa2 100%);">
                    Feed Forward
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);">
                    Add (Residual) + Dropout
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    Ø®Ø±ÙˆØ¬ÛŒ
                </div>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 12: Encoder Layer Ú©Ø§Ù…Ù„ -->
        <div class="slide">
            <div class="slide-number">12</div>
            <h2>ğŸ”¨ Ø³Ø§Ø®Øª ÛŒÚ© Encoder Layer Ú©Ø§Ù…Ù„</h2>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ¯</span> ØªØ±Ú©ÛŒØ¨ ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§</h3>
                <p>Ø­Ø§Ù„Ø§ ØªÙ…Ø§Ù… Ù‚Ø·Ø¹Ø§Øª Ø±Ø§ Ú©Ù†Ø§Ø± Ù‡Ù… Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… ØªØ§ ÛŒÚ© Ù„Ø§ÛŒÙ‡ Encoder Ú©Ø§Ù…Ù„ Ø¨Ø³Ø§Ø²ÛŒÙ….</p>
            </div>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">EncoderLayer</span>(nn.Module):
    <span class="string">"""
    ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ú©Ø§Ù…Ù„ Encoder Ø´Ø§Ù…Ù„:
    1. Multi-Head Self-Attention
    2. Feed Forward Network
    3. Layer Normalization & Residual Connections
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Multi-Head Attention</span>
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        <span class="comment"># Feed Forward Network</span>
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        
        <span class="comment"># Ø¯Ùˆ Sublayer Connection (Ø¨Ø±Ø§ÛŒ attention Ùˆ ffn)</span>
        self.sublayer1 = SublayerConnection(d_model, dropout)
        self.sublayer2 = SublayerConnection(d_model, dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        <span class="string">"""
        x: ÙˆØ±ÙˆØ¯ÛŒ [batch_size, seq_len, d_model]
        mask: Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ padding
        """</span>
        <span class="comment"># 1. Self-Attention + Add & Norm</span>
        x = self.sublayer1(x, <span class="keyword">lambda</span> x: self.self_attn(x, mask))
        
        <span class="comment"># 2. Feed Forward + Add & Norm</span>
        x = self.sublayer2(x, self.feed_forward)
        
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">Encoder</span>(nn.Module):
    <span class="string">"""
    Encoder Ú©Ø§Ù…Ù„ Ø¨Ø§ N Ù„Ø§ÛŒÙ‡
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model=<span class="number">512</span>, num_layers=<span class="number">6</span>, 
                 num_heads=<span class="number">8</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>):
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Embedding Layer</span>
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        <span class="comment"># Positional Encoding</span>
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        <span class="comment"># N Ù„Ø§ÛŒÙ‡ Encoder</span>
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)
        ])
        
        <span class="comment"># Layer Norm Ù†Ù‡Ø§ÛŒÛŒ</span>
        self.norm = LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        <span class="string">"""
        x: Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª [batch_size, seq_len]
        """</span>
        <span class="comment"># 1. Embedding</span>
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        
        <span class="comment"># 2. Positional Encoding</span>
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        <span class="comment"># 3. Ø¹Ø¨ÙˆØ± Ø§Ø² N Ù„Ø§ÛŒÙ‡ Encoder</span>
        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:
            x = layer(x, mask)
        
        <span class="comment"># 4. Normalization Ù†Ù‡Ø§ÛŒÛŒ</span>
        <span class="keyword">return</span> self.norm(x)

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
vocab_size = <span class="number">10000</span>
encoder = Encoder(vocab_size, d_model=<span class="number">512</span>, num_layers=<span class="number">6</span>)

<span class="comment"># Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª)</span>
x = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">32</span>, <span class="number">10</span>))  <span class="comment"># [batch=32, seq_len=10]</span>
output = encoder(x)
<span class="keyword">print</span>(<span class="string">f"Encoder output shape: {output.shape}"</span>)  <span class="comment"># [32, 10, 512]</span>
            </div>
            
            <div class="success box">
                <h4><span class="emoji">âœ…</span> Encoder Ú©Ø§Ù…Ù„ Ø´Ø¯!</h4>
                <p>Ø§ÛŒÙ† Encoder Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯:</p>
                <ul>
                    <li>Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ø¯</li>
                    <li>Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¯Ø±Ú© Ú©Ù†Ø¯</li>
                    <li>Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¯Ø± Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ø¯</li>
                    <li>Ø®Ø±ÙˆØ¬ÛŒ ØºÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 13: Decoder -->
        <div class="slide">
            <div class="slide-number">13</div>
            <h2>ğŸ¨ Decoder: ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ¤”</span> Decoder Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŸ</h3>
                <p>Decoder ÙˆØ¸ÛŒÙÙ‡ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ:</p>
                <ul>
                    <li><strong>Encoder:</strong> Ø¬Ù…Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯</li>
                    <li><strong>Decoder:</strong> ØªØ±Ø¬Ù…Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                </ul>
            </div>
            
            <div class="warning box">
                <h3><span class="emoji">âš ï¸</span> ØªÙØ§ÙˆØª Ø§ØµÙ„ÛŒ Ø¨Ø§ Encoder</h3>
                <p>Decoder Ø³Ù‡ Ù†ÙˆØ¹ Attention Ø¯Ø§Ø±Ø¯:</p>
                <ol>
                    <li><strong>Masked Self-Attention:</strong> ÙÙ‚Ø· Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù†Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡!)</li>
                    <li><strong>Encoder-Decoder Attention:</strong> Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Encoder ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                    <li><strong>Feed Forward:</strong> Ù…Ø«Ù„ Encoder</li>
                </ol>
            </div>
            
            <div class="diagram">
                <h4 style="color: #667eea; text-align: center; margin-bottom: 20px;">Ø³Ø§Ø®ØªØ§Ø± Decoder Layer</h4>
                
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    ÙˆØ±ÙˆØ¯ÛŒ Decoder
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);">
                    Masked Self-Attention<br>
                    <span style="font-size: 0.8em;">(ÙÙ‚Ø· Ø¨Ù‡ Ú¯Ø°Ø´ØªÙ‡ Ù†Ú¯Ø§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)</span>
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Add & Norm</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #2196f3 0%, #1976d2 100%);">
                    Encoder-Decoder Attention<br>
                    <span style="font-size: 0.8em;">(Ø¨Ù‡ Encoder ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)</span>
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Add & Norm</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #9c27b0 0%, #7b1fa2 100%);">
                    Feed Forward
                </div>
                <div class="arrow">â†“</div>
                <div class="layer-box">Add & Norm</div>
                <div class="arrow">â†“</div>
                <div class="layer-box" style="background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);">
                    Ø®Ø±ÙˆØ¬ÛŒ
                </div>
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ”‘</span> Masked Attention Ú†ÛŒØ³ØªØŸ</h4>
                <p><strong>Ù…Ø´Ú©Ù„:</strong> Ø¯Ø± Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯ØŒ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡ Ù†Ú¯Ø§Ù‡ Ú©Ù†ÛŒÙ… (Ú†ÙˆÙ† Ù‡Ù†ÙˆØ² ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯!)</p>
                <p><strong>Ø±Ø§Ù‡â€ŒØ­Ù„:</strong> Ø¨Ø§ ÛŒÚ© Ù…Ø§Ø³Ú© Ù…Ø«Ù„Ø«ÛŒØŒ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ø±Ø§ Ù…Ø³Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….</p>
                
                <div style="background: white; padding: 20px; border-radius: 10px; margin-top: 15px;">
                    <pre style="direction: ltr; text-align: left; font-family: monospace;">
Ø¬Ù…Ù„Ù‡: "Ù…Ù† ÛŒÚ© Ø¯Ø§Ù†Ø´Ø¬Ùˆ Ù‡Ø³ØªÙ…"

Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯ "Ù…Ù†":     âœ… Ù…Ù†
Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯ "ÛŒÚ©":     âœ… Ù…Ù†  âœ… ÛŒÚ©
Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯ "Ø¯Ø§Ù†Ø´Ø¬Ùˆ":  âœ… Ù…Ù†  âœ… ÛŒÚ©  âœ… Ø¯Ø§Ù†Ø´Ø¬Ùˆ
Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯ "Ù‡Ø³ØªÙ…":   âœ… Ù…Ù†  âœ… ÛŒÚ©  âœ… Ø¯Ø§Ù†Ø´Ø¬Ùˆ  âœ… Ù‡Ø³ØªÙ…
                    </pre>
                </div>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 14: Ú©Ø¯ Decoder -->
        <div class="slide">
            <div class="slide-number">14</div>
            <h2>ğŸ’» Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Decoder</h2>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">DecoderLayer</span>(nn.Module):
    <span class="string">"""
    ÛŒÚ© Ù„Ø§ÛŒÙ‡ Decoder Ø´Ø§Ù…Ù„:
    1. Masked Self-Attention
    2. Encoder-Decoder Attention
    3. Feed Forward Network
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Masked Self-Attention</span>
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        <span class="comment"># Encoder-Decoder Attention</span>
        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)
        
        <span class="comment"># Feed Forward</span>
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        
        <span class="comment"># Ø³Ù‡ Sublayer Connection</span>
        self.sublayer1 = SublayerConnection(d_model, dropout)
        self.sublayer2 = SublayerConnection(d_model, dropout)
        self.sublayer3 = SublayerConnection(d_model, dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, encoder_output, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        <span class="string">"""
        x: ÙˆØ±ÙˆØ¯ÛŒ decoder
        encoder_output: Ø®Ø±ÙˆØ¬ÛŒ encoder
        src_mask: Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ encoder
        tgt_mask: Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ decoder (Ù…Ø«Ù„Ø«ÛŒ)
        """</span>
        <span class="comment"># 1. Masked Self-Attention</span>
        x = self.sublayer1(x, <span class="keyword">lambda</span> x: self.self_attn(x, tgt_mask))
        
        <span class="comment"># 2. Encoder-Decoder Attention</span>
        <span class="comment"># Query Ø§Ø² decoderØŒ Key Ùˆ Value Ø§Ø² encoder</span>
        x = self.sublayer2(x, <span class="keyword">lambda</span> x: self.enc_dec_attn(
            x, encoder_output, encoder_output, src_mask
        ))
        
        <span class="comment"># 3. Feed Forward</span>
        x = self.sublayer3(x, self.feed_forward)
        
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">Decoder</span>(nn.Module):
    <span class="string">"""
    Decoder Ú©Ø§Ù…Ù„
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model=<span class="number">512</span>, num_layers=<span class="number">6</span>,
                 num_heads=<span class="number">8</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>):
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Embedding</span>
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        <span class="comment"># Positional Encoding</span>
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        <span class="comment"># N Ù„Ø§ÛŒÙ‡ Decoder</span>
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)
        ])
        
        <span class="comment"># Layer Norm Ù†Ù‡Ø§ÛŒÛŒ</span>
        self.norm = LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, encoder_output, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        <span class="comment"># 1. Embedding</span>
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        
        <span class="comment"># 2. Positional Encoding</span>
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        <span class="comment"># 3. Ø¹Ø¨ÙˆØ± Ø§Ø² N Ù„Ø§ÛŒÙ‡ Decoder</span>
        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        
        <span class="comment"># 4. Normalization Ù†Ù‡Ø§ÛŒÛŒ</span>
        <span class="keyword">return</span> self.norm(x)

<span class="keyword">def</span> <span class="function">create_masks</span>(src, tgt, pad_idx=<span class="number">0</span>):
    <span class="string">"""
    Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ padding Ùˆ future positions
    """</span>
    <span class="comment"># Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ padding Ø¯Ø± source</span>
    src_mask = (src != pad_idx).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)
    
    <span class="comment"># Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ target (Ù…Ø«Ù„Ø«ÛŒ)</span>
    tgt_len = tgt.size(<span class="number">1</span>)
    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).bool()
    tgt_mask = tgt_mask.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)
    
    <span class="comment"># Ù…Ø§Ø³Ú© padding Ø¯Ø± target</span>
    tgt_pad_mask = (tgt != pad_idx).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)
    tgt_mask = tgt_mask & tgt_pad_mask
    
    <span class="keyword">return</span> src_mask, tgt_mask
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ“</span> Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:</h4>
                <ul>
                    <li><strong>Masked Attention:</strong> Ø¨Ø§ Ù…Ø§Ø³Ú© Ù…Ø«Ù„Ø«ÛŒ Ø§Ø² Ø¯ÛŒØ¯Ù† Ø¢ÛŒÙ†Ø¯Ù‡ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…</li>
                    <li><strong>Encoder-Decoder Attention:</strong> Query Ø§Ø² decoderØŒ Key Ùˆ Value Ø§Ø² encoder</li>
                    <li><strong>Autoregressive:</strong> Ø®Ø±ÙˆØ¬ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 15: Transformer Ú©Ø§Ù…Ù„ -->
        <div class="slide">
            <div class="slide-number">15</div>
            <h2>ğŸ‰ Transformer Ú©Ø§Ù…Ù„</h2>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ†</span> ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ</h3>
                <p>Ø­Ø§Ù„Ø§ Encoder Ùˆ Decoder Ø±Ø§ Ú©Ù†Ø§Ø± Ù‡Ù… Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… ØªØ§ ÛŒÚ© Transformer Ú©Ø§Ù…Ù„ Ø¨Ø³Ø§Ø²ÛŒÙ…!</p>
            </div>
            
            <div class="code">
<span class="keyword">class</span> <span class="function">Transformer</span>(nn.Module):
    <span class="string">"""
    Ù…Ø¯Ù„ Transformer Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, src_vocab_size, tgt_vocab_size,
                 d_model=<span class="number">512</span>, num_layers=<span class="number">6</span>, num_heads=<span class="number">8</span>,
                 d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>):
        <span class="keyword">super</span>().__init__()
        
        <span class="comment"># Encoder</span>
        self.encoder = Encoder(
            src_vocab_size, d_model, num_layers,
            num_heads, d_ff, dropout, max_len
        )
        
        <span class="comment"># Decoder</span>
        self.decoder = Decoder(
            tgt_vocab_size, d_model, num_layers,
            num_heads, d_ff, dropout, max_len
        )
        
        <span class="comment"># Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ (ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ú©Ù„Ù…Ø§Øª)</span>
        self.output_layer = nn.Linear(d_model, tgt_vocab_size)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, src, tgt, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        <span class="string">"""
        src: Ø¬Ù…Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ [batch_size, src_len]
        tgt: Ø¬Ù…Ù„Ù‡ Ù‡Ø¯Ù [batch_size, tgt_len]
        """</span>
        <span class="comment"># 1. Encoder</span>
        encoder_output = self.encoder(src, src_mask)
        
        <span class="comment"># 2. Decoder</span>
        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)
        
        <span class="comment"># 3. Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ</span>
        output = self.output_layer(decoder_output)
        
        <span class="keyword">return</span> output
    
    <span class="keyword">def</span> <span class="function">generate</span>(self, src, max_len=<span class="number">50</span>, start_token=<span class="number">1</span>, end_token=<span class="number">2</span>):
        <span class="string">"""
        ØªÙˆÙ„ÛŒØ¯ ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª autoregressive
        """</span>
        self.eval()
        <span class="keyword">with</span> torch.no_grad():
            <span class="comment"># Encode Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ</span>
            src_mask, _ = create_masks(src, src)
            encoder_output = self.encoder(src, src_mask)
            
            <span class="comment"># Ø´Ø±ÙˆØ¹ Ø¨Ø§ start token</span>
            tgt = torch.LongTensor([[start_token]])
            
            <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len):
                <span class="comment"># Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ target</span>
                _, tgt_mask = create_masks(src, tgt)
                
                <span class="comment"># Decode</span>
                decoder_output = self.decoder(
                    tgt, encoder_output, src_mask, tgt_mask
                )
                
                <span class="comment"># Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ</span>
                output = self.output_layer(decoder_output[:, -<span class="number">1</span>, :])
                next_token = output.argmax(dim=-<span class="number">1</span>).unsqueeze(<span class="number">0</span>)
                
                <span class="comment"># Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ target</span>
                tgt = torch.cat([tgt, next_token], dim=<span class="number">1</span>)
                
                <span class="comment"># Ø§Ú¯Ø± Ø¨Ù‡ end token Ø±Ø³ÛŒØ¯ÛŒÙ…ØŒ ØªÙˆÙ‚Ù</span>
                <span class="keyword">if</span> next_token.item() == end_token:
                    <span class="keyword">break</span>
            
            <span class="keyword">return</span> tgt

<span class="comment"># Ø³Ø§Ø®Øª Ù…Ø¯Ù„</span>
src_vocab_size = <span class="number">10000</span>  <span class="comment"># ÙˆØ§Ú˜Ú¯Ø§Ù† Ø²Ø¨Ø§Ù† Ù…Ø¨Ø¯Ø§</span>
tgt_vocab_size = <span class="number">8000</span>   <span class="comment"># ÙˆØ§Ú˜Ú¯Ø§Ù† Ø²Ø¨Ø§Ù† Ù…Ù‚ØµØ¯</span>

model = Transformer(
    src_vocab_size, 
    tgt_vocab_size,
    d_model=<span class="number">512</span>,
    num_layers=<span class="number">6</span>,
    num_heads=<span class="number">8</span>
)

<span class="comment"># Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
src = torch.randint(<span class="number">0</span>, src_vocab_size, (<span class="number">32</span>, <span class="number">20</span>))  <span class="comment"># Ø¬Ù…Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ</span>
tgt = torch.randint(<span class="number">0</span>, tgt_vocab_size, (<span class="number">32</span>, <span class="number">15</span>))  <span class="comment"># Ø¬Ù…Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ</span>

src_mask, tgt_mask = create_masks(src, tgt)
output = model(src, tgt, src_mask, tgt_mask)

<span class="keyword">print</span>(<span class="string">f"Output shape: {output.shape}"</span>)  <span class="comment"># [32, 15, 8000]</span>
            </div>
            
            <div class="success box">
                <h4><span class="emoji">ğŸ‰</span> ØªØ¨Ø±ÛŒÚ©! Transformer Ú©Ø§Ù…Ù„ Ø´Ø¯!</h4>
                <p>Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯:</p>
                <ul>
                    <li>ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯</li>
                    <li>Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†</li>
                    <li>Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª</li>
                    <li>ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 16: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ -->
        <div class="slide">
            <div class="slide-number">16</div>
            <h2>ğŸ“ Ø¢Ù…ÙˆØ²Ø´ Transformer</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ“š</span> Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´</h3>
                <p>Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ TransformerØŒ Ø§Ø² ØªÚ©Ù†ÛŒÚ© <span class="highlight-text">Teacher Forcing</span> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….</p>
            </div>
            
            <div class="code">
<span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim

<span class="keyword">def</span> <span class="function">train_epoch</span>(model, train_loader, optimizer, criterion, device):
    <span class="string">"""
    Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© epoch
    """</span>
    model.train()
    total_loss = <span class="number">0</span>
    
    <span class="keyword">for</span> batch_idx, (src, tgt) <span class="keyword">in</span> enumerate(train_loader):
        src, tgt = src.to(device), tgt.to(device)
        
        <span class="comment"># tgt_input: ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¬Ø² Ø¢Ø®Ø±ÛŒ</span>
        <span class="comment"># tgt_output: ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¬Ø² Ø§ÙˆÙ„ÛŒ</span>
        tgt_input = tgt[:, :-<span class="number">1</span>]
        tgt_output = tgt[:, <span class="number">1</span>:]
        
        <span class="comment"># Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§</span>
        src_mask, tgt_mask = create_masks(src, tgt_input)
        
        <span class="comment"># Forward pass</span>
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask, tgt_mask)
        
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ loss</span>
        output = output.reshape(-<span class="number">1</span>, output.size(-<span class="number">1</span>))
        tgt_output = tgt_output.reshape(-<span class="number">1</span>)
        loss = criterion(output, tgt_output)
        
        <span class="comment"># Backward pass</span>
        loss.backward()
        
        <span class="comment"># Gradient Clipping (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² exploding gradients)</span>
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)
        
        optimizer.step()
        
        total_loss += loss.item()
        
        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>:
            <span class="keyword">print</span>(<span class="string">f"Batch {batch_idx}, Loss: {loss.item():.4f}"</span>)
    
    <span class="keyword">return</span> total_loss / len(train_loader)

<span class="comment"># ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´</span>
device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)
model = model.to(device)

<span class="comment"># Optimizer Ø¨Ø§ learning rate scheduling</span>
optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)

<span class="comment"># Loss function (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† padding)</span>
criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)

<span class="comment"># Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´</span>
num_epochs = <span class="number">10</span>
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):
    <span class="keyword">print</span>(<span class="string">f"\nEpoch {epoch + 1}/{num_epochs}"</span>)
    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
    <span class="keyword">print</span>(<span class="string">f"Average Loss: {train_loss:.4f}"</span>)
    
    <span class="comment"># Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„</span>
    torch.save(model.state_dict(), <span class="string">f"transformer_epoch_{epoch+1}.pt"</span>)
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ’¡</span> Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø¢Ù…ÙˆØ²Ø´:</h4>
                <ul>
                    <li><strong>Teacher Forcing:</strong> Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ØŒ Ú©Ù„Ù…Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… (Ù†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§)</li>
                    <li><strong>Gradient Clipping:</strong> Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² exploding gradients</li>
                    <li><strong>Label Smoothing:</strong> Ú©Ø§Ù‡Ø´ overfitting</li>
                    <li><strong>Warmup:</strong> Ø´Ø±ÙˆØ¹ Ø¨Ø§ learning rate Ú©Ù… Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¯Ø±ÛŒØ¬ÛŒ</li>
                </ul>
            </div>
            
            <div class="warning box">
                <h4><span class="emoji">âš ï¸</span> Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:</h4>
                <ul>
                    <li><strong>Ø­Ø§ÙØ¸Ù‡ GPU:</strong> Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø­Ø§ÙØ¸Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù†Ø¯</li>
                    <li><strong>Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´:</strong> Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø±ÙˆØ²Ù‡Ø§ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯</li>
                    <li><strong>Ø¯Ø§Ø¯Ù‡:</strong> Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…ÛŒÙ„ÛŒÙˆÙ†â€ŒÙ‡Ø§ Ø¬Ù…Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø®ÙˆØ¨</li>
                    <li><strong>Hyperparameter Tuning:</strong> ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø³Ø®Øª Ø§Ø³Øª</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 17: Ø§Ù†ÙˆØ§Ø¹ Transformer -->
        <div class="slide">
            <div class="slide-number">17</div>
            <h2>ğŸŒŸ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Transformer</h2>
            
            <div class="grid">
                <div class="card">
                    <h3><span class="emoji">ğŸ“–</span> BERT</h3>
                    <p><strong>Bidirectional Encoder</strong></p>
                    <ul>
                        <li>ÙÙ‚Ø· Ø§Ø² Encoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li>Ø¯ÙˆØ·Ø±ÙÙ‡ Ù…ØªÙ† Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯</li>
                        <li>Ø¨Ø±Ø§ÛŒ: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ NERØŒ QA</li>
                        <li>Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´: Masked Language Modeling</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h3><span class="emoji">âœï¸</span> GPT</h3>
                    <p><strong>Autoregressive Decoder</strong></p>
                    <ul>
                        <li>ÙÙ‚Ø· Ø§Ø² Decoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li>ÛŒÚ©â€ŒØ·Ø±ÙÙ‡ (Ú†Ù¾ Ø¨Ù‡ Ø±Ø§Ø³Øª)</li>
                        <li>Ø¨Ø±Ø§ÛŒ: ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ Ù…Ú©Ø§Ù„Ù…Ù‡</li>
                        <li>Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´: Next Token Prediction</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h3><span class="emoji">ğŸ”„</span> T5</h3>
                    <p><strong>Text-to-Text Transfer</strong></p>
                    <ul>
                        <li>Ø§Ø² Encoder Ùˆ Decoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li>Ù‡Ù…Ù‡ ØªØ³Ú©â€ŒÙ‡Ø§ Ø±Ø§ text-to-text Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li>Ø¨Ø±Ø§ÛŒ: ØªØ±Ø¬Ù…Ù‡ØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒØŒ QA</li>
                        <li>Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´: Span Corruption</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h3><span class="emoji">ğŸ¨</span> Vision Transformer</h3>
                    <p><strong>ViT</strong></p>
                    <ul>
                        <li>Transformer Ø¨Ø±Ø§ÛŒ ØªØµØ§ÙˆÛŒØ±</li>
                        <li>ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ù‡ patch ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li>Ø¨Ø±Ø§ÛŒ: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØµÙˆÛŒØ±</li>
                        <li>Ø±Ù‚ÛŒØ¨ CNN Ù‡Ø§</li>
                    </ul>
                </div>
            </div>
            
            <div class="timeline">
                <div class="timeline-item">
                    <span class="timeline-year">2017</span>
                    <h4>Transformer Ø§ØµÙ„ÛŒ</h4>
                    <p>Ù…Ù‚Ø§Ù„Ù‡ "Attention Is All You Need" - Google</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2018</span>
                    <h4>BERT</h4>
                    <p>Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø¯Ø± NLP Ø¨Ø§ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø¯ÙˆØ·Ø±ÙÙ‡ - Google</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2019</span>
                    <h4>GPT-2</h4>
                    <p>ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ - OpenAI</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2020</span>
                    <h4>GPT-3</h4>
                    <p>175 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±ØŒ Few-shot Learning - OpenAI</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2022</span>
                    <h4>ChatGPT</h4>
                    <p>Ù…Ø¯Ù„ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø§ RLHF - OpenAI</p>
                </div>
                
                <div class="timeline-item">
                    <span class="timeline-year">2023-2024</span>
                    <h4>GPT-4, Claude, Gemini</h4>
                    <p>Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡</p>
                </div>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 18: Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ -->
        <div class="slide">
            <div class="slide-number">18</div>
            <h2>ğŸš€ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Transformer</h2>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸŒ</span> ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ</h4>
                    <p>ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨ÛŒÙ† Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Google Translate</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’¬</span> Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§</h4>
                    <p>Ù…Ú©Ø§Ù„Ù…Ù‡ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§ Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> ChatGPT, Claude, Bard</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“</span> Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ</h4>
                    <p>ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ø§Ø² Ù…ØªÙˆÙ† Ø¨Ù„Ù†Ø¯</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Ø®Ù„Ø§ØµÙ‡ Ø§Ø®Ø¨Ø§Ø±ØŒ Ù…Ù‚Ø§Ù„Ø§Øª</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">â“</span> Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„</h4>
                    <p>Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø² Ø±ÙˆÛŒ Ù…ØªÙ†</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ·ï¸</span> Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ†</h4>
                    <p>Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¯</span> Named Entity Recognition</h4>
                    <p>Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ù… Ø§Ø´Ø®Ø§ØµØŒ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§ØŒ Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’»</span> ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯</h4>
                    <p>Ù†ÙˆØ´ØªÙ† Ú©Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> GitHub Copilot</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ–¼ï¸</span> ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ±</h4>
                    <p>ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ù…ØªÙ†ÛŒ</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> DALL-E, Midjourney</p>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸµ</span> ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ</h4>
                    <p>Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø¨Ø§ Transformer</p>
                    <p><strong>Ù…Ø«Ø§Ù„:</strong> Jukebox</p>
                </div>
            </div>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸŒ</span> ØªØ£Ø«ÛŒØ± Ø¨Ø± ØµÙ†Ø¹Øª</h3>
                <p>Transformer ØµÙ†Ø§ÛŒØ¹ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ù…ØªØ­ÙˆÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª:</p>
                <ul>
                    <li><strong>Ø®Ø¯Ù…Ø§Øª Ù…Ø´ØªØ±ÛŒ:</strong> Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯</li>
                    <li><strong>Ø¢Ù…ÙˆØ²Ø´:</strong> Ø¯Ø³ØªÛŒØ§Ø±Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø®ØµÛŒ</li>
                    <li><strong>Ù¾Ø²Ø´Ú©ÛŒ:</strong> ØªØ­Ù„ÛŒÙ„ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ù¾Ø²Ø´Ú©ÛŒ</li>
                    <li><strong>Ø­Ù‚ÙˆÙ‚ÛŒ:</strong> Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§</li>
                    <li><strong>ØªØ­Ù‚ÛŒÙ‚:</strong> Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ</li>
                    <li><strong>ØªØ¬Ø§Ø±Øª:</strong> ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø± Ùˆ Ù…Ø´ØªØ±ÛŒ</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 19: Ù…Ø²Ø§ÛŒØ§ Ùˆ Ù…Ø¹Ø§ÛŒØ¨ -->
        <div class="slide">
            <div class="slide-number">19</div>
            <h2>âš–ï¸ Ù…Ø²Ø§ÛŒØ§ Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Transformer</h2>
            
            <div class="comparison">
                <div class="success box">
                    <h3><span class="emoji">âœ…</span> Ù…Ø²Ø§ÛŒØ§</h3>
                    <ul>
                        <li><strong>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ:</strong> Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø² RNN/LSTM</li>
                        <li><strong>Long-range Dependencies:</strong> ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÙˆØ± Ø±Ø§ Ø¨Ù‡ØªØ± ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯</li>
                        <li><strong>Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ:</strong> Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>
                        <li><strong>Transfer Learning:</strong> Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ùˆ fine-tuning Ø¢Ø³Ø§Ù†</li>
                        <li><strong>Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ:</strong> Ù…ØªÙ†ØŒ ØªØµÙˆÛŒØ±ØŒ ØµØ¯Ø§ Ø±Ø§ Ø¨Ø§ Ù‡Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li><strong>State-of-the-art:</strong> Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø§Ú©Ø«Ø± ØªØ³Ú©â€ŒÙ‡Ø§</li>
                    </ul>
                </div>
                
                <div class="warning box">
                    <h3><span class="emoji">âš ï¸</span> Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§</h3>
                    <ul>
                        <li><strong>Ø­Ø§ÙØ¸Ù‡:</strong> O(nÂ²) Ø¨Ø±Ø§ÛŒ self-attention (Ù…Ø´Ú©Ù„ Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† Ø¨Ù„Ù†Ø¯)</li>
                        <li><strong>Ù…Ø­Ø§Ø³Ø¨Ø§Øª:</strong> Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU/TPU Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯</li>
                        <li><strong>Ø¯Ø§Ø¯Ù‡:</strong> Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´</li>
                        <li><strong>Interpretability:</strong> Ø³Ø®Øª Ø§Ø³Øª Ø¨ÙÙ‡Ù…ÛŒÙ… Ú†Ø·ÙˆØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li><strong>Ù‡Ø²ÛŒÙ†Ù‡:</strong> Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú¯Ø±Ø§Ù† Ø§Ø³Øª</li>
                        <li><strong>Ø·ÙˆÙ„ Ù…ØªÙ†:</strong> Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø± Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 512-4096 ØªÙˆÚ©Ù†)</li>
                    </ul>
                </div>
            </div>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ”¬</span> ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¬Ø§Ø±ÛŒ</h3>
                <p>Ù…Ø­Ù‚Ù‚Ø§Ù† Ø¯Ø± Ø­Ø§Ù„ Ú©Ø§Ø± Ø±ÙˆÛŒ Ø­Ù„ Ø§ÛŒÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ Ù‡Ø³ØªÙ†Ø¯:</p>
                <ul>
                    <li><strong>Efficient Transformers:</strong> Ú©Ø§Ù‡Ø´ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ O(nÂ²) Ø¨Ù‡ O(n log n) ÛŒØ§ O(n)</li>
                    <li><strong>Sparse Attention:</strong> ØªÙˆØ¬Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…</li>
                    <li><strong>Longer Context:</strong> Ø§ÙØ²Ø§ÛŒØ´ Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ 100K+ ØªÙˆÚ©Ù†</li>
                    <li><strong>Smaller Models:</strong> Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨</li>
                    <li><strong>Quantization:</strong> Ú©Ø§Ù‡Ø´ Ø­Ø¬Ù… Ù…Ø¯Ù„ Ø¨Ø§ Ø­ÙØ¸ Ø¯Ù‚Øª</li>
                </ul>
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ’¡</span> Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:</h4>
                <p>Ø¨Ø§ ÙˆØ¬ÙˆØ¯ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ØŒ Transformer Ù‡Ù…Ú†Ù†Ø§Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ NLP Ùˆ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª. ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¬Ø§Ø±ÛŒ Ø¨Ù‡ Ø³Ø±Ø¹Øª Ø¯Ø± Ø­Ø§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù‡Ø³ØªÙ†Ø¯.</p>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 20: Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ -->
        <div class="slide">
            <div class="slide-number">20</div>
            <h2>ğŸ› ï¸ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ¤—</span> Hugging Face Transformers</h3>
                <p>Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù…Ø­Ø¨ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø³Ø§Ù† Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Transformer Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø´Ø¯Ù‡</p>
            </div>
            
            <div class="code">
<span class="comment"># Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡</span>
pip install transformers torch

<span class="comment"># Ù…Ø«Ø§Ù„ 1: ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª</span>
<span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline

<span class="comment"># Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„</span>
classifier = pipeline(<span class="string">"sentiment-analysis"</span>)

<span class="comment"># Ø§Ø³ØªÙØ§Ø¯Ù‡</span>
result = classifier(<span class="string">"I love this product!"</span>)
<span class="keyword">print</span>(result)
<span class="comment"># Output: [{'label': 'POSITIVE', 'score': 0.9998}]</span>

<span class="comment"># Ù…Ø«Ø§Ù„ 2: ØªØ±Ø¬Ù…Ù‡</span>
translator = pipeline(<span class="string">"translation_en_to_fr"</span>)
result = translator(<span class="string">"Hello, how are you?"</span>)
<span class="keyword">print</span>(result)
<span class="comment"># Output: [{'translation_text': 'Bonjour, comment allez-vous?'}]</span>

<span class="comment"># Ù…Ø«Ø§Ù„ 3: ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†</span>
generator = pipeline(<span class="string">"text-generation"</span>, model=<span class="string">"gpt2"</span>)
result = generator(
    <span class="string">"Once upon a time"</span>,
    max_length=<span class="number">50</span>,
    num_return_sequences=<span class="number">1</span>
)
<span class="keyword">print</span>(result[<span class="number">0</span>][<span class="string">'generated_text'</span>])

<span class="comment"># Ù…Ø«Ø§Ù„ 4: Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„</span>
qa = pipeline(<span class="string">"question-answering"</span>)
context = <span class="string">"""
Transformer is a deep learning architecture developed by Google.
It was introduced in 2017 in the paper "Attention Is All You Need".
"""</span>
question = <span class="string">"When was Transformer introduced?"</span>

result = qa(question=question, context=context)
<span class="keyword">print</span>(result)
<span class="comment"># Output: {'answer': '2017', 'score': 0.98}</span>

<span class="comment"># Ù…Ø«Ø§Ù„ 5: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ù…Ø¯Ù„</span>
<span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel
<span class="keyword">import</span> torch

<span class="comment"># Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ tokenizer Ùˆ Ù…Ø¯Ù„</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)
model = AutoModel.from_pretrained(<span class="string">"bert-base-uncased"</span>)

<span class="comment"># Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ</span>
text = <span class="string">"Hello, my name is John."</span>

<span class="comment"># Tokenization</span>
inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>)

<span class="comment"># Forward pass</span>
<span class="keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="comment"># Ø¯Ø±ÛŒØ§ÙØª embeddings</span>
last_hidden_states = outputs.last_hidden_state
<span class="keyword">print</span>(<span class="string">f"Shape: {last_hidden_states.shape}"</span>)
<span class="comment"># Output: Shape: torch.Size([1, 8, 768])</span>

<span class="comment"># Ù…Ø«Ø§Ù„ 6: Fine-tuning Ø¨Ø±Ø§ÛŒ ØªØ³Ú© Ø®Ø§Øµ</span>
<span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments
<span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset

<span class="comment"># Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ dataset</span>
dataset = load_dataset(<span class="string">"imdb"</span>)

<span class="comment"># Tokenization</span>
<span class="keyword">def</span> <span class="function">tokenize_function</span>(examples):
    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="keyword">True</span>)

tokenized_datasets = dataset.map(tokenize_function, batched=<span class="keyword">True</span>)

<span class="comment"># ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´</span>
training_args = TrainingArguments(
    output_dir=<span class="string">"./results"</span>,
    num_train_epochs=<span class="number">3</span>,
    per_device_train_batch_size=<span class="number">16</span>,
    save_steps=<span class="number">1000</span>,
    save_total_limit=<span class="number">2</span>,
)

<span class="comment"># Trainer</span>
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[<span class="string">"train"</span>],
    eval_dataset=tokenized_datasets[<span class="string">"test"</span>],
)

<span class="comment"># Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´</span>
trainer.train()
            </div>
            
            <div class="success box">
                <h4><span class="emoji">ğŸ‰</span> Ù…Ø²Ø§ÛŒØ§ÛŒ Hugging Face:</h4>
                <ul>
                    <li>Ø¯Ø³ØªØ±Ø³ÛŒ Ø¢Ø³Ø§Ù† Ø¨Ù‡ ØµØ¯Ù‡Ø§ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø´Ø¯Ù‡</li>
                    <li>API Ø³Ø§Ø¯Ù‡ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡</li>
                    <li>Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² PyTorch Ùˆ TensorFlow</li>
                    <li>Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Fine-tuning Ø¢Ù…Ø§Ø¯Ù‡</li>
                    <li>Ø¬Ø§Ù…Ø¹Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ ÙØ¹Ø§Ù„</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 21: Ù†Ú©Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ -->
        <div class="slide">
            <div class="slide-number">21</div>
            <h2>ğŸ“ Ù†Ú©Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡</h2>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸ”§</span> Optimization Tricks</h4>
                    <ul>
                        <li><strong>Learning Rate Warmup:</strong> Ø´Ø±ÙˆØ¹ Ø¨Ø§ LR Ú©Ù…</li>
                        <li><strong>Label Smoothing:</strong> Ú©Ø§Ù‡Ø´ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯</li>
                        <li><strong>Gradient Accumulation:</strong> Ø¨Ø±Ø§ÛŒ batch size Ø¨Ø²Ø±Ú¯â€ŒØªØ±</li>
                        <li><strong>Mixed Precision:</strong> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² FP16 Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“Š</span> Attention Patterns</h4>
                    <ul>
                        <li><strong>Local Attention:</strong> ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§</li>
                        <li><strong>Global Attention:</strong> ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªÙ…Ø§Ù… ØªÙˆÚ©Ù†â€ŒÙ‡Ø§</li>
                        <li><strong>Sparse Attention:</strong> ØªÙˆØ¬Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ</li>
                        <li><strong>Sliding Window:</strong> Ù¾Ù†Ø¬Ø±Ù‡ Ù…ØªØ­Ø±Ú©</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¯</span> Pre-training Tasks</h4>
                    <ul>
                        <li><strong>MLM:</strong> Masked Language Modeling</li>
                        <li><strong>NSP:</strong> Next Sentence Prediction</li>
                        <li><strong>CLM:</strong> Causal Language Modeling</li>
                        <li><strong>Denoising:</strong> Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ØªÙ†</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">âš¡</span> Efficient Architectures</h4>
                    <ul>
                        <li><strong>Linformer:</strong> O(n) complexity</li>
                        <li><strong>Reformer:</strong> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LSH</li>
                        <li><strong>Longformer:</strong> Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† Ø¨Ù„Ù†Ø¯</li>
                        <li><strong>BigBird:</strong> ØªØ±Ú©ÛŒØ¨ attention patterns</li>
                    </ul>
                </div>
            </div>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ§ª</span> ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡</h3>
                
                <h4>1. Layer-wise Learning Rate Decay</h4>
                <div class="code">
<span class="comment"># Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ learning rate Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯</span>
optimizer_grouped_parameters = [
    {
        <span class="string">"params"</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() 
                    <span class="keyword">if</span> <span class="string">"layer.0"</span> <span class="keyword">in</span> n],
        <span class="string">"lr"</span>: <span class="number">1e-5</span>
    },
    {
        <span class="string">"params"</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() 
                    <span class="keyword">if</span> <span class="string">"layer.11"</span> <span class="keyword">in</span> n],
        <span class="string">"lr"</span>: <span class="number">1e-4</span>
    }
]
                </div>
                
                <h4>2. Attention Visualization</h4>
                <div class="code">
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">import</span> seaborn <span class="keyword">as</span> sns

<span class="keyword">def</span> <span class="function">visualize_attention</span>(attention_weights, tokens):
    <span class="string">"""Ù†Ù…Ø§ÛŒØ´ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention"""</span>
    plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))
    sns.heatmap(
        attention_weights.detach().numpy(),
        xticklabels=tokens,
        yticklabels=tokens,
        cmap=<span class="string">"viridis"</span>
    )
    plt.title(<span class="string">"Attention Weights"</span>)
    plt.show()
                </div>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 22: Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ -->
        <div class="slide">
            <div class="slide-number">22</div>
            <h2>ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨ÛŒØ´ØªØ±</h2>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸ“„</span> Ù…Ù‚Ø§Ù„Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ</h4>
                    <ul>
                        <li><strong>Attention Is All You Need</strong> (2017) - Ù…Ù‚Ø§Ù„Ù‡ Ø§ØµÙ„ÛŒ</li>
                        <li><strong>BERT</strong> (2018) - Pre-training of Deep Bidirectional Transformers</li>
                        <li><strong>GPT-3</strong> (2020) - Language Models are Few-Shot Learners</li>
                        <li><strong>Vision Transformer</strong> (2020) - An Image is Worth 16x16 Words</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“–</span> Ú©ØªØ§Ø¨â€ŒÙ‡Ø§</h4>
                    <ul>
                        <li><strong>Natural Language Processing with Transformers</strong> - O'Reilly</li>
                        <li><strong>Deep Learning</strong> - Goodfellow et al.</li>
                        <li><strong>Speech and Language Processing</strong> - Jurafsky & Martin</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¥</span> Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†</h4>
                    <ul>
                        <li><strong>Stanford CS224N:</strong> NLP with Deep Learning</li>
                        <li><strong>Hugging Face Course:</strong> Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¹Ø§Ù„ÛŒ</li>
                        <li><strong>Fast.ai:</strong> Practical Deep Learning</li>
                        <li><strong>DeepLearning.AI:</strong> NLP Specialization</li>
                    ```html
class="string">f'Batch {batch_idx}, Loss: {loss.item():.4f}'</span>)
    
    <span class="keyword">return</span> total_loss / len(train_loader)

<span class="keyword">def</span> <span class="function">train_model</span>(model, train_loader, val_loader, num_epochs=<span class="number">10</span>):
    <span class="string">"""
    Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„
    """</span>
    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
    model = model.to(device)
    
    <span class="comment"># Optimizer: Adam Ø¨Ø§ learning rate scheduling</span>
    optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)
    
    <span class="comment"># Learning Rate Scheduler</span>
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode=<span class="string">'min'</span>, factor=<span class="number">0.5</span>, patience=<span class="number">2</span>
    )
    
    <span class="comment"># Loss Function: CrossEntropyLoss Ø¨Ø§ ignore_index Ø¨Ø±Ø§ÛŒ padding</span>
    criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)
    
    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):
        <span class="keyword">print</span>(<span class="string">f'\n===== Epoch {epoch+1}/{num_epochs} ====='</span>)
        
        <span class="comment"># Ø¢Ù…ÙˆØ²Ø´</span>
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        <span class="comment"># Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</span>
        val_loss = evaluate(model, val_loader, criterion, device)
        
        <span class="keyword">print</span>(<span class="string">f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}'</span>)
        
        <span class="comment"># Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ learning rate</span>
        scheduler.step(val_loss)
        
        <span class="comment"># Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„</span>
        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:
            torch.save(model.state_dict(), <span class="string">f'transformer_epoch_{epoch}.pth'</span>)

<span class="keyword">def</span> <span class="function">evaluate</span>(model, val_loader, criterion, device):
    <span class="string">"""
    Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
    """</span>
    model.eval()
    total_loss = <span class="number">0</span>
    
    <span class="keyword">with</span> torch.no_grad():
        <span class="keyword">for</span> src, tgt <span class="keyword">in</span> val_loader:
            src, tgt = src.to(device), tgt.to(device)
            
            tgt_input = tgt[:, :-<span class="number">1</span>]
            tgt_output = tgt[:, <span class="number">1</span>:]
            
            src_mask, tgt_mask = create_masks(src, tgt_input)
            output = model(src, tgt_input, src_mask, tgt_mask)
            
            output = output.reshape(-<span class="number">1</span>, output.size(-<span class="number">1</span>))
            tgt_output = tgt_output.reshape(-<span class="number">1</span>)
            loss = criterion(output, tgt_output)
            
            total_loss += loss.item()
    
    <span class="keyword">return</span> total_loss / len(val_loader)
            </div>
            
            <div class="key-point">
                <h4><span class="emoji">ğŸ”‘</span> Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø¢Ù…ÙˆØ²Ø´:</h4>
                <ul>
                    <li><strong>Teacher Forcing:</strong> Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ (Ù†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…</li>
                    <li><strong>Gradient Clipping:</strong> Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² exploding gradients</li>
                    <li><strong>Learning Rate Scheduling:</strong> Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ learning rate</li>
                    <li><strong>Label Smoothing:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overconfidence</li>
                    <li><strong>Warmup:</strong> Ø§ÙØ²Ø§ÛŒØ´ ØªØ¯Ø±ÛŒØ¬ÛŒ learning rate Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 17: Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Transformer -->
        <div class="slide">
            <div class="slide-number">17</div>
            <h2>ğŸŒŸ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Transformer</h2>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸŒ</span> ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ</h4>
                    <p>ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨ÛŒÙ† Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù</p>
                    <ul>
                        <li>Google Translate</li>
                        <li>DeepL</li>
                        <li>Microsoft Translator</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’¬</span> Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§</h4>
                    <p>Ù…Ú©Ø§Ù„Ù…Ù‡ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†</p>
                    <ul>
                        <li>ChatGPT</li>
                        <li>Claude (Ù…Ù†!)</li>
                        <li>Google Bard</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“</span> Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ</h4>
                    <p>Ø®Ù„Ø§ØµÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªÙˆÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ</p>
                    <ul>
                        <li>Ø®Ù„Ø§ØµÙ‡ Ø§Ø®Ø¨Ø§Ø±</li>
                        <li>Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§Ù„Ø§Øª</li>
                        <li>Ø®Ù„Ø§ØµÙ‡ Ø¬Ù„Ø³Ø§Øª</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">â“</span> Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„</h4>
                    <p>Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†</p>
                    <ul>
                        <li>Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ</li>
                        <li>Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ</li>
                        <li>Ø¯Ø³ØªÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø¬Ø§Ø²ÛŒ</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ–¼ï¸</span> Vision Transformer</h4>
                    <p>Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ø¨Ø§ Transformer</p>
                    <ul>
                        <li>ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§</li>
                        <li>Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØµØ§ÙˆÛŒØ±</li>
                        <li>ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ±</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸµ</span> Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª</h4>
                    <p>Ú©Ø§Ø± Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ</p>
                    <ul>
                        <li>ØªØ¨Ø¯ÛŒÙ„ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ†</li>
                        <li>ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø±</li>
                        <li>ØªØ±Ø¬Ù…Ù‡ Ù‡Ù…Ø²Ù…Ø§Ù†</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ§¬</span> Ø¨ÛŒÙˆØ§Ù†ÙÙˆØ±Ù…Ø§ØªÛŒÚ©</h4>
                    <p>ØªØ­Ù„ÛŒÙ„ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ</p>
                    <ul>
                        <li>Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆØªØ¦ÛŒÙ†</li>
                        <li>ØªØ­Ù„ÛŒÙ„ DNA</li>
                        <li>Ú©Ø´Ù Ø¯Ø§Ø±Ùˆ</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’»</span> ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯</h4>
                    <p>Ú©Ù…Ú© Ø¨Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ</p>
                    <ul>
                        <li>GitHub Copilot</li>
                        <li>CodeWhisperer</li>
                        <li>Tabnine</li>
                    </ul>
                </div>
            </div>
            
            <div class="highlight box" style="margin-top: 30px;">
                <h3><span class="emoji">ğŸš€</span> Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ù‡ÙˆØ± Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Transformer</h3>
                <table>
                    <tr>
                        <th>Ù…Ø¯Ù„</th>
                        <th>Ø³Ø§Ø²Ù†Ø¯Ù‡</th>
                        <th>Ù…Ø¹Ù…Ø§Ø±ÛŒ</th>
                        <th>Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø§ØµÙ„ÛŒ</th>
                    </tr>
                    <tr>
                        <td><strong>BERT</strong></td>
                        <td>Google</td>
                        <td>Encoder-only</td>
                        <td>Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†ØŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ</td>
                    </tr>
                    <tr>
                        <td><strong>GPT-4</strong></td>
                        <td>OpenAI</td>
                        <td>Decoder-only</td>
                        <td>ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ Ù…Ú©Ø§Ù„Ù…Ù‡</td>
                    </tr>
                    <tr>
                        <td><strong>T5</strong></td>
                        <td>Google</td>
                        <td>Encoder-Decoder</td>
                        <td>Ù‡Ù…Ù‡ ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ NLP</td>
                    </tr>
                    <tr>
                        <td><strong>Claude</strong></td>
                        <td>Anthropic</td>
                        <td>Decoder-only</td>
                        <td>Ù…Ú©Ø§Ù„Ù…Ù‡ØŒ ØªØ­Ù„ÛŒÙ„</td>
                    </tr>
                    <tr>
                        <td><strong>LLaMA</strong></td>
                        <td>Meta</td>
                        <td>Decoder-only</td>
                        <td>ØªØ­Ù‚ÛŒÙ‚ØŒ Ù…ØªÙ†â€ŒØ¨Ø§Ø²</td>
                    </tr>
                    <tr>
                        <td><strong>ViT</strong></td>
                        <td>Google</td>
                        <td>Encoder-only</td>
                        <td>Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±</td>
                    </tr>
                </table>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 18: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ -->
        <div class="slide">
            <div class="slide-number">18</div>
            <h2>âš¡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ø±Ù†</h2>
            
            <div class="info box">
                <h3><span class="emoji">ğŸ”§</span> Ù…Ø´Ú©Ù„Ø§Øª Transformer Ø§ØµÙ„ÛŒ</h3>
                <ul>
                    <li><strong>Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§Ù„Ø§:</strong> Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù„Ù†Ø¯ØŒ Ø­Ø§ÙØ¸Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ù†ÛŒØ§Ø² Ø§Ø³Øª</li>
                    <li><strong>Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø³Ù†Ú¯ÛŒÙ†:</strong> Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ O(nÂ²) Ø¨Ø±Ø§ÛŒ self-attention</li>
                    <li><strong>Ø·ÙˆÙ„ Ù…Ø­Ø¯ÙˆØ¯:</strong> Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ù…ØªÙˆÙ† Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯ Ú©Ø§Ø± Ú©Ù†Ø¯</li>
                </ul>
            </div>
            
            <h3 style="margin-top: 30px;">ğŸš€ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù†ÙˆÛŒÙ†</h3>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">âš¡</span> Flash Attention</h4>
                    <p>Ù…Ø­Ø§Ø³Ø¨Ù‡ attention Ø¨Ø§ Ø³Ø±Ø¹Øª Ùˆ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù‡ØªØ±</p>
                    <ul>
                        <li>Ú©Ø§Ù‡Ø´ Ûµ-Û±Û° Ø¨Ø±Ø§Ø¨Ø±ÛŒ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡</li>
                        <li>Ø³Ø±Ø¹Øª Û²-Û³ Ø¨Ø±Ø§Ø¨Ø±ÛŒ</li>
                        <li>Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± Ø¯Ù‚Øª</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¯</span> Sparse Attention</h4>
                    <p>ØªÙˆØ¬Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª</p>
                    <ul>
                        <li>Longformer</li>
                        <li>BigBird</li>
                        <li>Ú©Ø§Ù‡Ø´ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø¨Ù‡ O(n)</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ”„</span> Rotary Positional Embedding</h4>
                    <p>Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ positional encoding</p>
                    <ul>
                        <li>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± GPT-Neo, LLaMA</li>
                        <li>Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±</li>
                        <li>ØªØ¹Ù…ÛŒÙ… Ø¨Ù‡ Ø·ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“Š</span> Mixed Precision Training</h4>
                    <p>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² FP16 Ø¨Ù‡ Ø¬Ø§ÛŒ FP32</p>
                    <ul>
                        <li>Ú©Ø§Ù‡Ø´ ÛµÛ°Ùª Ø­Ø§ÙØ¸Ù‡</li>
                        <li>Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±</li>
                        <li>Ø¯Ù‚Øª Ù…Ø´Ø§Ø¨Ù‡</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¨</span> Multi-Query Attention</h4>
                    <p>Ø§Ø´ØªØ±Ø§Ú© Key Ùˆ Value Ø¨ÛŒÙ† heads</p>
                    <ul>
                        <li>Ú©Ø§Ù‡Ø´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§</li>
                        <li>Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± inference</li>
                        <li>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± PaLM</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ”—</span> Sliding Window Attention</h4>
                    <p>ØªÙˆØ¬Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ Ù…Ø­Ù„ÛŒ</p>
                    <ul>
                        <li>Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† Ø¨Ù„Ù†Ø¯</li>
                        <li>Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø®Ø·ÛŒ</li>
                        <li>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Mistral</li>
                    </ul>
                </div>
            </div>
            
            <div class="code">
<span class="comment"># Ù…Ø«Ø§Ù„: Flash Attention (Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡)</span>
<span class="keyword">def</span> <span class="function">flash_attention</span>(Q, K, V, block_size=<span class="number">256</span>):
    <span class="string">"""
    Ù…Ø­Ø§Ø³Ø¨Ù‡ attention Ø¨Ù‡ ØµÙˆØ±Øª block-wise
    Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡
    """</span>
    seq_len = Q.size(<span class="number">1</span>)
    output = torch.zeros_like(Q)
    
    <span class="comment"># ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ block Ù‡Ø§</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, seq_len, block_size):
        end = min(i + block_size, seq_len)
        
        <span class="comment"># Ù…Ø­Ø§Ø³Ø¨Ù‡ attention Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† block</span>
        Q_block = Q[:, i:end, :]
        scores = torch.matmul(Q_block, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))
        scores = scores / math.sqrt(Q.size(-<span class="number">1</span>))
        
        attention = F.softmax(scores, dim=-<span class="number">1</span>)
        output[:, i:end, :] = torch.matmul(attention, V)
    
    <span class="keyword">return</span> output

<span class="comment"># Ù…Ø«Ø§Ù„: Rotary Position Embedding</span>
<span class="keyword">class</span> <span class="function">RotaryEmbedding</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, dim):
        <span class="keyword">super</span>().__init__()
        inv_freq = <span class="number">1.0</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).float() / dim))
        self.register_buffer(<span class="string">'inv_freq'</span>, inv_freq)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, seq_len):
        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
        freqs = torch.einsum(<span class="string">'i,j->ij'</span>, t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)
        <span class="keyword">return</span> emb.cos(), emb.sin()
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 19: Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ -->
        <div class="slide">
            <div class="slide-number">19</div>
            <h2>ğŸ’¡ Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§</h2>
            
            <div class="grid">
                <div class="success box">
                    <h4><span class="emoji">âœ…</span> Ø§Ù†ØªØ®Ø§Ø¨ Hyperparameters</h4>
                    <ul>
                        <li><strong>d_model:</strong> 512 ÛŒØ§ 768 (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©) ØªØ§ 1024-2048 (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯)</li>
                        <li><strong>num_layers:</strong> 6 (Ù¾Ø§ÛŒÙ‡)ØŒ 12 (Ù…ØªÙˆØ³Ø·)ØŒ 24+ (Ø¨Ø²Ø±Ú¯)</li>
                        <li><strong>num_heads:</strong> 8 ÛŒØ§ 16 (Ø¨Ø§ÛŒØ¯ d_model Ø¨Ø± Ø¢Ù† Ø¨Ø®Ø´â€ŒÙ¾Ø°ÛŒØ± Ø¨Ø§Ø´Ø¯)</li>
                        <li><strong>d_ff:</strong> Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 4 Ã— d_model</li>
                        <li><strong>dropout:</strong> 0.1 ØªØ§ 0.3</li>
                    </ul>
                </div>
                
                <div class="warning box">
                    <h4><span class="emoji">âš ï¸</span> Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø±Ø§ÛŒØ¬</h4>
                    <ul>
                        <li><strong>ÙØ±Ø§Ù…ÙˆØ´ÛŒ Positional Encoding:</strong> Ù…Ø¯Ù„ ØªØ±ØªÛŒØ¨ Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯</li>
                        <li><strong>Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Masking:</strong> Ù…Ø¯Ù„ Ø¨Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ù†Ú¯Ø§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯</li>
                        <li><strong>Learning Rate Ø¨Ø§Ù„Ø§:</strong> Ù…Ø¯Ù„ diverge Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>
                        <li><strong>Ø¨Ø¯ÙˆÙ† Warmup:</strong> Ø¢Ù…ÙˆØ²Ø´ Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø± Ø§Ø³Øª</li>
                        <li><strong>Gradient Clipping Ù†Ú©Ø±Ø¯Ù†:</strong> exploding gradients</li>
                    </ul>
                </div>
            </div>
            
            <div class="info box">
                <h4><span class="emoji">ğŸ¯</span> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Production</h4>
                <ul>
                    <li><strong>Quantization:</strong> ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ INT8 Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±</li>
                    <li><strong>Distillation:</strong> Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø²Ø±Ú¯</li>
                    <li><strong>Pruning:</strong> Ø­Ø°Ù ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª</li>
                    <li><strong>ONNX Export:</strong> Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù</li>
                    <li><strong>Caching:</strong> Ø°Ø®ÛŒØ±Ù‡ Key Ùˆ Value Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø³Ø±ÛŒØ¹â€ŒØªØ±</li>
                </ul>
            </div>
            
            <div class="code">
<span class="comment"># Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´</span>

<span class="comment"># 1. Warmup Learning Rate</span>
<span class="keyword">class</span> <span class="function">WarmupScheduler</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(self, optimizer, d_model, warmup_steps=<span class="number">4000</span>):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = <span class="number">0</span>
    
    <span class="keyword">def</span> <span class="function">step</span>(self):
        self.step_num += <span class="number">1</span>
        lr = self.d_model ** (-<span class="number">0.5</span>) * min(
            self.step_num ** (-<span class="number">0.5</span>),
            self.step_num * self.warmup_steps ** (-<span class="number">1.5</span>)
        )
        <span class="keyword">for</span> param_group <span class="keyword">in</span> self.optimizer.param_groups:
            param_group[<span class="string">'lr'</span>] = lr

<span class="comment"># 2. Label Smoothing</span>
<span class="keyword">class</span> <span class="function">LabelSmoothingLoss</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, smoothing=<span class="number">0.1</span>, vocab_size=<span class="number">10000</span>):
        <span class="keyword">super</span>().__init__()
        self.smoothing = smoothing
        self.vocab_size = vocab_size
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, pred, target):
        confidence = <span class="number">1.0</span> - self.smoothing
        smooth_value = self.smoothing / (self.vocab_size - <span class="number">1</span>)
        
        <span class="comment"># Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ²ÛŒØ¹ smooth</span>
        true_dist = torch.full_like(pred, smooth_value)
        true_dist.scatter_(<span class="number">1</span>, target.unsqueeze(<span class="number">1</span>), confidence)
        
        <span class="keyword">return</span> F.kl_div(F.log_softmax(pred, dim=-<span class="number">1</span>), true_dist, reduction=<span class="string">'batchmean'</span>)

<span class="comment"># 3. Gradient Accumulation (Ø¨Ø±Ø§ÛŒ batch size Ø¨Ø²Ø±Ú¯â€ŒØªØ±)</span>
accumulation_steps = <span class="number">4</span>
<span class="keyword">for</span> i, (src, tgt) <span class="keyword">in</span> enumerate(train_loader):
    output = model(src, tgt)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:
        optimizer.step()
        optimizer.zero_grad()
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 20: Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ -->
        <div class="slide">
            <div class="slide-number">20</div>
            <h2>ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ</h2>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ“š</span> Ú†ÛŒØ²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÛŒÙ…</h3>
                <ol style="font-size: 1.1em; line-height: 2.5;">
                    <li><strong>Self-Attention:</strong> Ù‚Ù„Ø¨ Transformer - ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªÙ…Ø§Ù… Ú©Ù„Ù…Ø§Øª Ù‡Ù…Ø²Ù…Ø§Ù†</li>
                    <li><strong>Multi-Head Attention:</strong> ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§ Ú†Ù†Ø¯ÛŒÙ† head</li>
                    <li><strong>Positional Encoding:</strong> Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¨Ù‡ Ù…Ø¯Ù„</li>
                    <li><strong>Feed Forward Network:</strong> Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø§Ø·Ù„Ø§Ø¹Ø§Øª</li>
                    <li><strong>Encoder-Decoder:</strong> Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ sequence-to-sequence</li>
                    <li><strong>Residual Connections:</strong> Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² vanishing gradient</li>
                    <li><strong>Layer Normalization:</strong> Ù¾Ø§ÛŒØ¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´</li>
                </ol>
            </div>
            
            <div class="grid" style="margin-top: 30px;">
                <div class="success box">
                    <h4><span class="emoji">âœ…</span> Ù…Ø²Ø§ÛŒØ§ÛŒ Transformer</h4>
                    <ul>
                        <li>Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ú©Ø§Ù…Ù„</li>
                        <li>Ø¯Ø±Ú© Ø±ÙˆØ§Ø¨Ø· Ø¯ÙˆØ± Ø¯Ø± Ù…ØªÙ†</li>
                        <li>Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¹Ø§Ù„ÛŒ</li>
                        <li>Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø±ØªØ± Ø¯Ø± ØªÙ…Ø§Ù… ØªØ³Ú©â€ŒÙ‡Ø§</li>
                        <li>Ù‚Ø§Ø¨Ù„ÛŒØª transfer learning</li>
                    </ul>
                </div>
                
                <div class="warning box">
                    <h4><span class="emoji">âš ï¸</span> Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§</h4>
                    <ul>
                        <li>Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø²ÛŒØ§Ø¯</li>
                        <li>Ø·ÙˆÙ„ Ù…Ø­Ø¯ÙˆØ¯ ÙˆØ±ÙˆØ¯ÛŒ</li>
                        <li>Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´</li>
                        <li>ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ÛŒ Ú©Ù…</li>
                        <li>Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø§Ù„Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´</li>
                    </ul>
                </div>
            </div>
            
            <div class="info box" style="margin-top: 30px;">
                <h3><span class="emoji">ğŸš€</span> Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ</h3>
                <ul>
                    <li><strong>Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„ÛŒ:</strong> ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ø§ Transformer Ø¨Ø³Ø§Ø²ÛŒØ¯</li>
                    <li><strong>Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª:</strong> "Attention Is All You Need" Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒØ¯</li>
                    <li><strong>Ú©Ø§Ø± Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø´Ø¯Ù‡:</strong> BERTØŒ GPT Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯</li>
                    <li><strong>Fine-tuning:</strong> Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Ú© Ø®Ø§Øµ Ø®ÙˆØ¯ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯</li>
                    <li><strong>Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:</strong> ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±ÛŒØ¯</li>
                </ul>
            </div>
            
            <div class="key-point" style="margin-top: 30px;">
                <h3 style="text-align: center;"><span class="emoji">ğŸ’¡</span> Ù†Ú©ØªÙ‡ Ø·Ù„Ø§ÛŒÛŒ</h3>
                <p style="text-align: center; font-size: 1.3em; font-weight: 600;">
                    "Transformer ØªÙ†Ù‡Ø§ ÛŒÚ© Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ù„Ú©Ù‡ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ù†Ù‚Ù„Ø§Ø¨ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø¯Ø±Ù† Ø¨Ø± Ø¢Ù† Ø¨Ù†Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª."
                </p>
            </div>
            
            <div style="text-align: center; margin-top: 50px; padding: 40px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 20px;">
                <h2 style="color: white; border: none; margin-bottom: 20px;"><span class="emoji">ğŸ‰</span> ØªØ¨Ø±ÛŒÚ©!</h2>
                <p style="font-size: 1.4em; line-height: 2;">
                    Ø´Ù…Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ø¹Ù…Ø§Ø±ÛŒ Transformer Ø±Ø§ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÛŒØ¯!
                </p>
                <p style="font-size: 1.2em; margin-top: 20px;">
                    Ø­Ø§Ù„Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ§ÛŒØ¯ ØªØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ NLP Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†ÙˆØ¢ÙˆØ±ÛŒ Ú©Ù†ÛŒØ¯!
                </p>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 21: Ù…Ù†Ø§Ø¨Ø¹ -->
        <div class="slide">
            <div class="slide-number">21</div>
            <h2>ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ ÙˆÙ…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ±</h2>
            
            <div class="grid">
                <div class="card">
                    <h4><span class="emoji">ğŸ“„</span> Ù…Ù‚Ø§Ù„Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ</h4>
                    <ul>
                        <li><strong>"Attention Is All You Need"</strong> (Vaswani et al., 2017) - Ù…Ù‚Ø§Ù„Ù‡ Ø§ØµÙ„ÛŒ Transformer</li>
                        <li><strong>"BERT: Pre-training of Deep Bidirectional Transformers"</strong> (Devlin et al., 2018)</li>
                        <li><strong>"Language Models are Few-Shot Learners"</strong> (Brown et al., 2020) - GPT-3</li>
                        <li><strong>"An Image is Worth 16x16 Words"</strong> (Dosovitskiy et al., 2020) - Vision Transformer</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“–</span> Ú©ØªØ§Ø¨â€ŒÙ‡Ø§</h4>
                    <ul>
                        <li><strong>"Natural Language Processing with Transformers"</strong> - Lewis Tunstall et al.</li>
                        <li><strong>"Deep Learning"</strong> - Ian Goodfellow et al.</li>
                        <li><strong>"Speech and Language Processing"</strong> - Dan Jurafsky & James H. Martin</li>
                        <li><strong>"Dive into Deep Learning"</strong> - Aston Zhang et al.</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ¥</span> Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†</h4>
                    <ul>
                        <li><strong>Stanford CS224N:</strong> Natural Language Processing with Deep Learning</li>
                        <li><strong>Hugging Face Course:</strong> Ø¯ÙˆØ±Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¬Ø§Ù…Ø¹</li>
                        <li><strong>Fast.ai:</strong> Practical Deep Learning for Coders</li>
                        <li><strong>DeepLearning.AI:</strong> Natural Language Processing Specialization</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ’»</span> Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§</h4>
                    <ul>
                        <li><strong>Hugging Face Transformers:</strong> Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§ØµÙ„ÛŒ</li>
                        <li><strong>PyTorch:</strong> ÙØ±ÛŒÙ…ÙˆØ±Ú© deep learning</li>
                        <li><strong>TensorFlow:</strong> ÙØ±ÛŒÙ…ÙˆØ±Ú© Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†</li>
                        <li><strong>JAX:</strong> Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ù¾ÛŒØ´Ø±ÙØªÙ‡</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸŒ</span> ÙˆØ¨â€ŒØ³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯</h4>
                    <ul>
                        <li><strong>Papers With Code:</strong> Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§ Ú©Ø¯ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ</li>
                        <li><strong>ArXiv:</strong> Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù‚ÛŒÙ‚Ø§Øª</li>
                        <li><strong>Towards Data Science:</strong> Ù…Ù‚Ø§Ù„Ø§Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ</li>
                        <li><strong>Distill.pub:</strong> ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨ØµØ±ÛŒ</li>
                    </ul>
                </div>
                
                <div class="card">
                    <h4><span class="emoji">ğŸ“</span> Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø±Ø§Ú©Ø² ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ</h4>
                    <ul>
                        <li><strong>Stanford NLP Group</strong></li>
                        <li><strong>MIT CSAIL</strong></li>
                        <li><strong>Google AI Research</strong></li>
                        <li><strong>OpenAI</strong></li>
                    </ul>
                </div>
            </div>
            
            <div class="info box" style="margin-top: 30px;">
                <h3><span class="emoji">ğŸ”—</span> Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…</h3>
                <table style="margin-top: 20px;">
                    <tr>
                        <th>Ù…Ù†Ø¨Ø¹</th>
                        <th>ØªÙˆØ¶ÛŒØ­Ø§Øª</th>
                        <th>Ø³Ø·Ø­</th>
                    </tr>
                    <tr>
                        <td><strong>Illustrated Transformer</strong></td>
                        <td>ØªÙˆØ¶ÛŒØ­ ØªØµÙˆÛŒØ±ÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ø² Jay Alammar</td>
                        <td>Ù…Ø¨ØªØ¯ÛŒ</td>
                    </tr>
                    <tr>
                        <td><strong>Annotated Transformer</strong></td>
                        <td>Ú©Ø¯ Ú©Ø§Ù…Ù„ Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø®Ø· Ø¨Ù‡ Ø®Ø·</td>
                        <td>Ù…ØªÙˆØ³Ø·</td>
                    </tr>
                    <tr>
                        <td><strong>Hugging Face Documentation</strong></td>
                        <td>Ù…Ø³ØªÙ†Ø¯Ø§Øª Ú©Ø§Ù…Ù„ Ùˆ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ</td>
                        <td>Ù‡Ù…Ù‡ Ø³Ø·ÙˆØ­</td>
                    </tr>
                    <tr>
                        <td><strong>PyTorch Transformer Tutorial</strong></td>
                        <td>Ø¢Ù…ÙˆØ²Ø´ Ø±Ø³Ù…ÛŒ PyTorch</td>
                        <td>Ù…ØªÙˆØ³Ø·</td>
                    </tr>
                    <tr>
                        <td><strong>Attention Mechanism Papers</strong></td>
                        <td>Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø·</td>
                        <td>Ù¾ÛŒØ´Ø±ÙØªÙ‡</td>
                    </tr>
                </table>
            </div>
            
            <div class="success box" style="margin-top: 30px;">
                <h3><span class="emoji">ğŸ¯</span> Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø±ÛŒÙ†</h3>
                <ol style="font-size: 1.1em;">
                    <li><strong>ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ Ø³Ø§Ø¯Ù‡:</strong> Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ dataset Ú©ÙˆÚ†Ú©</li>
                    <li><strong>Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†:</strong> Ø®Ù„Ø§ØµÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø®Ø¨Ø§Ø± ÙØ§Ø±Ø³ÛŒ</li>
                    <li><strong>Ú†Øªâ€ŒØ¨Ø§Øª Ø³Ø§Ø¯Ù‡:</strong> Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…ØªØ¯Ø§ÙˆÙ„</li>
                    <li><strong>ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª:</strong> Ù…Ø«Ø¨Øª/Ù…Ù†ÙÛŒ Ø¨ÙˆØ¯Ù† Ù†Ø¸Ø±Ø§Øª</li>
                    <li><strong>ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†:</strong> Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù† Ø¯Ø§Ø³ØªØ§Ù†</li>
                    <li><strong>Named Entity Recognition:</strong> Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ù… Ø§Ø´Ø®Ø§Øµ Ùˆ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§</li>
                </ol>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ 22: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ -->
        <div class="slide">
            <div class="slide-number">22</div>
            <h2>ğŸ› ï¸ Ú©Ø¯ Ú©Ø§Ù…Ù„ Transformer (Ø®Ù„Ø§ØµÙ‡)</h2>
            
            <div class="highlight box">
                <h3><span class="emoji">ğŸ“¦</span> Ú©Ø¯ Ú©Ø§Ù…Ù„ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§</h3>
                <p>Ø§ÛŒÙ† Ú©Ø¯ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ Transformer Ø§Ø³Øª Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:</p>
            </div>
            
            <div class="code">
<span class="comment">"""
Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Transformer Ø§Ø² Ø§Ø¨ØªØ¯Ø§
Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡: Ø¢Ù…ÙˆØ²Ø´ Transformer
"""</span>

<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F
<span class="keyword">import</span> math

<span class="comment"># ===================== Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ =====================</span>

<span class="keyword">class</span> <span class="function">PositionalEncoding</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, max_len=<span class="number">5000</span>):
        <span class="keyword">super</span>().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)
        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * 
                           -(math.log(<span class="number">10000.0</span>) / d_model))
        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)
        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)
        self.register_buffer(<span class="string">'pe'</span>, pe.unsqueeze(<span class="number">0</span>))
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="keyword">return</span> x + self.pe[:, :x.size(<span class="number">1</span>)]

<span class="keyword">class</span> <span class="function">MultiHeadAttention</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads):
        <span class="keyword">super</span>().__init__()
        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    <span class="keyword">def</span> <span class="function">split_heads</span>(self, x):
        batch_size, seq_len, d_model = x.size()
        <span class="keyword">return</span> x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, query, key, value, mask=<span class="keyword">None</span>):
        Q = self.split_heads(self.W_q(query))
        K = self.split_heads(self.W_k(key))
        V = self.split_heads(self.W_v(value))
        
        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)
        
        <span class="keyword">if</span> mask <span class="keyword">is not None</span>:
            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)
        
        attention = F.softmax(scores, dim=-<span class="number">1</span>)
        output = torch.matmul(attention, V)
        
        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()
        output = output.view(output.size(<span class="number">0</span>), -<span class="number">1</span>, self.d_model)
        
        <span class="keyword">return</span> self.W_o(output)

<span class="keyword">class</span> <span class="function">FeedForward</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="keyword">return</span> self.linear2(self.dropout(F.relu(self.linear1(x))))

<span class="keyword">class</span> <span class="function">EncoderLayer</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">Encoder</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):
        <span class="keyword">super</span>().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, mask=<span class="keyword">None</span>):
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:
            x = layer(x, mask)
        
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">DecoderLayer</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, d_model, num_heads, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, enc_output, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))
        
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">Decoder</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):
        <span class="keyword">super</span>().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x, enc_output, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:
            x = layer(x, enc_output, src_mask, tgt_mask)
        
        <span class="keyword">return</span> x

<span class="keyword">class</span> <span class="function">Transformer</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, src_vocab_size, tgt_vocab_size, d_model=<span class="number">512</span>,
                 num_layers=<span class="number">6</span>, num_heads=<span class="number">8</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>):
        <span class="keyword">super</span>().__init__()
        self.encoder = Encoder(src_vocab_size, d_model, num_layers, 
                              num_heads, d_ff, dropout, max_len)
        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers,
                              num_heads, d_ff, dropout, max_len)
        self.output_layer = nn.Linear(d_model, tgt_vocab_size)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, src, tgt, src_mask=<span class="keyword">None</span>, tgt_mask=<span class="keyword">None</span>):
        enc_output = self.encoder(src, src_mask)
        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)
        output = self.output_layer(dec_output)
        <span class="keyword">return</span> output

<span class="comment"># ===================== Ø§Ø³ØªÙØ§Ø¯Ù‡ =====================</span>

<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§</span>
    src_vocab_size = <span class="number">10000</span>
    tgt_vocab_size = <span class="number">8000</span>
    d_model = <span class="number">512</span>
    num_layers = <span class="number">6</span>
    num_heads = <span class="number">8</span>
    d_ff = <span class="number">2048</span>
    
    <span class="comment"># Ø³Ø§Ø®Øª Ù…Ø¯Ù„</span>
    model = Transformer(src_vocab_size, tgt_vocab_size, d_model,
                       num_layers, num_heads, d_ff)
    
    <span class="comment"># Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡</span>
    src = torch.randint(<span class="number">0</span>, src_vocab_size, (<span class="number">32</span>, <span class="number">20</span>))
    tgt = torch.randint(<span class="number">0</span>, tgt_vocab_size, (<span class="number">32</span>, <span class="number">15</span>))
    
    <span class="comment"># Forward pass</span>
    output = model(src, tgt)
    
    <span class="keyword">print</span>(<span class="string">f"Input shape: {src.shape}"</span>)
    <span class="keyword">print</span>(<span class="string">f"Output shape: {output.shape}"</span>)
    <span class="keyword">print</span>(<span class="string">f"Model parameters: {sum(p.numel() for p in model.parameters()):,}"</span>)
    
    <span class="keyword">print</span>(<span class="string">"\nâœ… Transformer successfully created!"</span>)
            </div>
            
            <div class="success box">
                <h4><span class="emoji">ğŸ‰</span> Ø§ÛŒÙ† Ú©Ø¯ Ø´Ø§Ù…Ù„:</h4>
                <ul>
                    <li>âœ… Positional Encoding</li>
                    <li>âœ… Multi-Head Attention</li>
                    <li>âœ… Feed Forward Network</li>
                    <li>âœ… Encoder Ùˆ Decoder Ú©Ø§Ù…Ù„</li>
                    <li>âœ… Layer Normalization</li>
                    <li>âœ… Residual Connections</li>
                    <li>âœ… Masking</li>
                </ul>
            </div>
        </div>

        <!-- Ø§Ø³Ù„Ø§ÛŒØ¯ Ù¾Ø§ÛŒØ§Ù†ÛŒ -->
        <div class="slide">
            <div class="slide-number">23</div>
            
            <div style="text-align: center; padding: 60px 20px;">
                <h1 style="font-size: 3.5em; margin-bottom: 30px; color: #667eea;">
                    <span class="emoji" style="font-size: 1.2em;">ğŸ“</span><br>
                    Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´ Transformer
                </h1>
                
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 50px; border-radius: 25px; margin: 40px 0; box-shadow: 0 20px 60px rgba(0,0,0,0.3);">
                    <h2 style="color: white; border: none; font-size: 2.5em; margin-bottom: 25px;">
                        <span class="emoji">ğŸŒŸ</span> ØªØ¨Ø±ÛŒÚ©!
                    </h2>
                    <p style="font-size: 1.5em; line-height: 2.2; margin-bottom: 30px;">
                        Ø´Ù…Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø±Ø§ ÙØ±Ø§ Ú¯Ø±ÙØªÛŒØ¯!
                    </p>
                    <p style="font-size: 1.3em; line-height: 2;">
                        Ø­Ø§Ù„Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ NLP Ø¨Ø³Ø§Ø²ÛŒØ¯ØŒ<br>
                        Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø²Ø¨Ø§Ù†ÛŒ Ú©Ø§Ø± Ú©Ù†ÛŒØ¯ØŒ<br>
                        Ùˆ Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†ÙˆØ¢ÙˆØ±ÛŒ Ú©Ù†ÛŒØ¯!
                    </p>
                </div>
                
                <div class="grid" style="margin-top: 50px;">
                    <div class="card" style="text-align: center;">
                        <h3><span class="emoji">ğŸ“š</span></h3>
                        <h4>Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…Ø³ØªÙ…Ø±</h4>
                        <p>Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†ÛŒØ¯</p>
                    </div>
                    
                    <div class="card" style="text-align: center;">
                        <h3><span class="emoji">ğŸ’»</span></h3>
                        <h4>ØªÙ…Ø±ÛŒÙ† Ø¹Ù…Ù„ÛŒ</h4>
                        <p>Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯</p>
                    </div>
                    
                    <div class="card" style="text-align: center;">
                        <h3><span class="emoji">ğŸ¤</span></h3>
                        <h4>Ù‡Ù…Ú©Ø§Ø±ÛŒ</h4>
                        <p>Ø¨Ø§ Ø¬Ø§Ù…Ø¹Ù‡ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ø´ÛŒØ¯</p>
                    </div>
                    
                    <div class="card" style="text-align: center;">
                        <h3><span class="emoji">ğŸš€</span></h3>
                        <h4>Ù†ÙˆØ¢ÙˆØ±ÛŒ</h4>
                        <p>Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯</p>
                    </div>
                </div>
                
                <div class="highlight box" style="margin-top: 50px; text-align: center;">
                    <h3 style="font-size: 2em; margin-bottom: 20px;">
                        <span class="emoji">ğŸ’¡</span> ÛŒØ§Ø¯ØªØ§Ù† Ø¨Ø§Ø´Ø¯
                    </h3>
                    <p style="font-size: 1.4em; line-height: 2.2; font-weight: 600;">
                        "Ø¨Ù‡ØªØ±ÛŒÙ† Ø±Ø§Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒØŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù† Ø§Ø³Øª.<br>
                        Ù¾Ø³ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ØŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ú©Ù†ÛŒØ¯ØŒ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±ÛŒØ¯ØŒ Ùˆ Ù¾ÛŒØ´Ø±ÙØª Ú©Ù†ÛŒØ¯!"
                    </p>
                </div>
                
                <div style="margin-top: 60px; padding: 40px; background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); border-radius: 20px; border: 3px solid #4caf50;">
                    <h3 style="color: #2e7d32; font-size: 2em; margin-bottom: 20px;">
                        <span class="emoji">ğŸ“§</span> ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø§
                    </h3>
                    <p style="font-size: 1.2em; color: #1b5e20;">
                        Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§ØªØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§ØªØŒ ÛŒØ§ Ù‡Ù…Ú©Ø§Ø±ÛŒ:<br>
                        <strong>ÙˆØ¨â€ŒØ³Ø§ÛŒØª:</strong> araz.me<br>
                        <strong>Ø³Ø§ÛŒØª Ø¢Ù…ÙˆØ²Ø´ÛŒ:</strong> gistalk.ir
                    </p>
                </div>
                
                <div style="margin-top: 50px; font-size: 1.3em; color: #667eea;">
                    <p><strong>Ø¢Ø±Ø²ÙˆÛŒ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§! ğŸ‰</strong></p>
                    <p style="margin-top: 15px;">Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ â¤ï¸ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±Ù†Ø¯Ú¯Ø§Ù† Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯</p>
                </div>
            </div>
        </div>

    </div>
    
    <script>
        // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù†ÛŒÙ…ÛŒØ´Ù† Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù„Ø§ÛŒØ¯Ù‡Ø§
        document.addEventListener('DOMContentLoaded', function() {
            const slides = document.querySelectorAll('.slide');
            
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.opacity = '0';
                        entry.target.style.transform = 'translateY(20px)';
                        
                        setTimeout(() => {
                            entry.target.style.transition = 'all 0.6s ease-out';
                            entry.target.style.opacity = '1';
                            entry.target.style.transform = 'translateY(0)';
                        }, 100);
                    }
                });
            }, {
                threshold: 0.1
            });
            
            slides.forEach(slide => {
                observer.observe(slide);
            });
            
            // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† smooth scroll
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    const target = document.querySelector(this.getAttribute('href'));
                    if (target) {
                        target.scrollIntoView({
                            behavior: 'smooth',
                            block: 'start'
                        });
                    }
                });
            });
            
            console.log('ğŸ‰ Ø¢Ù…ÙˆØ²Ø´ Transformer Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!');
            console.log('ğŸ“š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø§ÛŒØ¯Ù‡Ø§:', slides.length);
        });
    </script>
</body>
</html>


